{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The Open Health Stack's Analytics components provide a scalable and flexible collection of tools to transform complex HL7 FHIR data into formats for running analytics workloads and building downstream applications.</p> <p>Using OHS, developers can use familiar languages and tools to build analytics solutions for different use cases: from generating reports and powering dashboards to exploratory data science and machine learning.</p> <p></p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li> <p>Apache Beam based ETL pipelines to continuously   transform FHIR resources to \"near lossless\" 'FHIR-in-Parquet' representation   based on a   natural schema   for projecting FHIR resource to Parquet.</p> </li> <li> <p>Pipelines Controller module provides pipeline management and scheduling   capabilities.</p> </li> <li> <p>Flexible deployment modes to meet the needs of different projects and teams   from simple single machine to multi-worker horizontally scalable distributed   environments. With support for local, on-prem or cloud based runners.</p> </li> <li> <p>Support for different target databases including traditional RDBMS (such as   PostgreSQL) or any OLAP engines that can load   Parquet files (such as SparkSQL or   DuckDB).</p> </li> <li> <p>Simplify querying data by defining views in SQL or as   ViewDefinition   resources to create flattened tables. Easily build analytics applications with   common languages (e.g. SQL, python) and BI or data visualizations tools ( e.g.   Apache Superset).</p> </li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<ul> <li> <p>The primary use case for FHIR Data Pipes is to enable continuous   transformation of FHIR data into analytics friendly representations to make it   easier for developers to: build dashboards, generate reports, perform data   science task, and create features for machine learning models.</p> </li> <li> <p>A secondary use case is for piping FHIR data from a FHIR source to another   FHIR server e.g. for integration into a central FHIR repository.</p> </li> </ul>"},{"location":"additional/","title":"Additional topics","text":""},{"location":"additional/#authentication","title":"Authentication","text":"<p>Any openIDConnect provider can be used to supply oAuth credentials.</p> <p>The required set of parameters are:</p> <ul> <li>oidConnectUrl</li> <li>clientId</li> <li>clientSecret</li> <li>oAuthUsername</li> <li>oAuthPassword</li> </ul> <p>You can add oAuth authentication to the pipeline by providing the required set of parameters via the command line or in the <code>/pipelines/controller/config/application.yaml</code> file.</p> Cmd line <p><code>java $ java -cp ./pipelines/batch/target/batch-bundled-0.1.0-SNAPSHOT.jar org.openmrs.analytics.FhirEtl \\ --fhirServerUrl=[FHIR_SERVER_URL] --outputParquetPath=[PATH] \\ --resourceList=Patient,Encounter,Observation --batchSize=200 \\ --clientId=[CLIENT_ID] --clientSecret=[CLIENT_SECRET] \\ --OAuthUsername=[USERNAME] --OAuthPassword=[PASSSWORD] \\ --oidConnectUrl= [OPENID_CONNECT_URL] \\</code></p> application.yaml <p><code>fhirdata: ..... # The following client credentials should be set if the FHIR server accepts # OAuth access tokens. Note the client credentials, e.g., the secret, are # sensitive, and it is probably a better practice to set these through # command-line arguments. fhirServerOAuthTokenEndpoint: \"https://path_to_endpoint_for_token\" fhirServerOAuthClientId: \"THE_CLIENT_ID\" fhirServerOAuthClientSecret: \"THE_CLIENT_SECRET\"</code></p>"},{"location":"additional/#config-properties","title":"Config properties","text":"<p>The main configuration for the FHIR Data Pipes Pipeline and Controller is the <code>/pipelines/controller/config/application.yaml</code> file which is well documented.</p> <p>When using the provided docker images, this will be found in <code>/docker/config.application.yaml</code></p>"},{"location":"additional/#parquet-on-fhir-schema","title":"Parquet on FHIR schema","text":"<p>Apache Parquet is a horizontally scalable columnar format that is optimized for performance.</p> <p>FHIR Data Pipes transforms FHIR resources to \"near lossless\" 'Parquet on FHIR' representation based on the \"Simplified SQL Projection of FHIR Resources\" ( 'SQL-on-FHIR-v1') schema</p> <ul> <li> <p>The conversion is done using a forked version of   Bunsen library   to transform from FHIR (current support for STU3, R4) to the SQL-on-FHIR-v1   schema</p> </li> <li> <p>The conversion is done by going from StructureDefinition --&gt; AvroConverter --&gt;   Parquet</p> </li> <li> <p>Configurable support for FHIR versions, profiles and extensions is provided</p> </li> </ul>"},{"location":"additional/#monitoring-pipelines","title":"Monitoring pipelines","text":"<p>The pipelines controller exposes management end-points that can help with monitoring the health of pipelines.</p> <ul> <li>The application has been integrated with the Spring Boot Actuator of Spring   and has exposed Rest API end points for monitoring, health checks, metrics   etc.</li> <li>The end points can be customised in the configuration file.</li> <li>It can easily be integrated with tools like Prometheus for monitoring metrics.</li> </ul> <p>Via the Web Control Panel The Web Control panel provides a quick glimpse about the latest state of the application including:</p> <ul> <li>Controls for triggering pipeline run on-demand</li> <li>A readable view of the application configuration</li> <li>Location and time of the latest snapshot created by the pipeline run</li> <li>Metrics of the most recent pipeline</li> <li>Error logs of the last pipeline if any</li> </ul> <p>These are found in the <code>application.yaml</code> config file in the <code>management:</code> section.</p> <p>See Config properties</p>"},{"location":"additional/#web-control-panel","title":"Web Control Panel","text":"<p>The web control panel is a basic spring application provided to make interacting with the pipeline controller easier.</p> <p>It is not designed to be a full production ready \u201cweb admin\u201d panel.</p> <p>The web control panel has the following features:</p> <ul> <li>Initiate full and incremental pipeline runs</li> <li>Monitor errors when running pipelines</li> <li>Recreate view tables</li> <li>View configuration settings</li> <li>Access sample jupyter notebooks and ViewDefinition editor</li> </ul> <p></p>"},{"location":"contributing/","title":"How to Contribute","text":"<p>We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.</p>"},{"location":"contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>Contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one.</p> <p>You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.</p>"},{"location":"contributing/#code-reviews","title":"Code reviews","text":"<p>All submissions by non-project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. We use GitHub for issue tracking.</p>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<p>This project follows Google's Open Source Community Guidelines.</p>"},{"location":"getting_started/","title":"Start Here","text":"<p>FHIR Data Pipes can be installed and deployed on a number of different platforms and using different architectural patterns depending on the environment, amount of data and requirements for horizontal scalability.</p> <p>Installation options include:</p> <ul> <li>Via docker images</li> <li>Deploy Data Pipes \"Pipelines\"</li> <li>Deploy Data Pipes \"Pipelines Controller\"</li> </ul> <p>Quick Start Guide</p> <p>The easiest way to get started is to follow the Single Machine Deployment tutorial. This uses a set of docker images and provided sample data to bring up an end-to-end environment for getting started with FHIR Data Pipes on a single machine</p>"},{"location":"installation_controller/","title":"Deploy the Controller Module","text":"<p>Guide Overview</p> <ul> <li>This guide will walk you through how to deploy and run the Pipelines Controller Module</li> <li>This module is designed to work with the FHIR Data Pipes ETL Pipelines</li> <li>For ease of deployment a set of example docker compose configurations that include the Pipeline and Controller has been provided. See the Docker section</li> </ul>"},{"location":"installation_controller/#usage","title":"Usage","text":"<p>The FHIR Pipelines Controller helps you schedule and manage the running of the FHIR Data Pipes Pipeline. Using the Controller module you can configure the Pipeline to run either full or incremental transformations, and it can also be used to monitor the pipelines.</p> <p>The Controller acts as a simplified entry point to managing the Data Pipes Pipelines for less experienced users and non-developers, all the configurations and features supported by the controller are also available by running the ETL Pipelines as a Java JAR.</p>"},{"location":"installation_controller/#setup","title":"Setup","text":"<ol> <li>Clone the    fhir-data-pipes GitHub repository,    open a terminal window.</li> <li><code>cd</code> to the directory where you cloned it.</li> <li>Change to the <code>controller</code> directory: <code>cd pipelines/controller/</code>.</li> </ol> <p>Later terminal commands will assume your working directory is the <code>controller</code> directory.</p> <p></p> <p>Next, configure the FHIR Pipelines Controller. The controller relies on several configuration files to run. Edit them to match your environment and requirements. For sample values that are used in single machine deployment examples see docker/config.</p> <ul> <li>Main settings for Pipeline Controller:   <code>pipelines/controller/config/application.yaml</code>.   Edit the values to match your FHIR server and use case.</li> <li>JDBC settings:   <code>pipelines/controller/config/hapi-postgres-config.json</code>.   If you are using the JDBC mode to read resources, edit the values to match the   database of your HAPI FHIR server. The JDBC mode is not implemented for other   FHIR servers.</li> <li>Customized FlinkRunner settings:   <code>pipelines/controller/config/flink-conf.yaml</code>.   To use this file, set the   <code>FLINK_CONF_DIR</code> environmental variable:   <code>export FLINK_CONF_DIR=[PATH]/flink-conf.yaml</code>. Changing   <code>taskmanager.memory.network.max</code> is necessary to avoid memory errors for large   datasets or when running on machines with limited memory.</li> <li>Spark Server settings:   <code>pipelines/controller/config/thriftserver-hive-config.json</code>.   Used to connect to a Spark server when   <code>createHiveResourceTables</code>   is set to <code>true</code>. Edit the values to match your Spark server if necessary.</li> </ul>"},{"location":"installation_controller/#run-the-fhir-pipelines-controller","title":"Run the FHIR Pipelines Controller","text":"<p>The Pipelines Controller is run from the FHIR Pipelines Control Panel.</p> <p>Warning</p> <p>Note: The Pipelines Controller is not intended to be a security barrier. As such the control panel does not support user authorization or any advanced security features.</p> <p>There are 2 ways to run the FHIR Pipelines Controller from the web application control panel.</p> <p>Using Spring Boot:</p> <pre><code>mvn spring-boot:run\n</code></pre> <p>Running the JAR directly:</p> <pre><code>mvn clean install\njava -jar ./target/controller-bundled.jar\n</code></pre> <p>After running, open a web browser and visit http://localhost:8080. You should see the FHIR Pipelines Control Panel.</p> <p>There are 3 ways to have the FHIR Pipelines Controller run the transformation pipeline:</p> <ul> <li>Manually trigger the Run Full option by clicking the button. This   transforms all the selected FHIR resource types to Parquet files. You are   required to use this option once before using any of the following   incremental options.</li> <li>Manually trigger the Run Incremental option by clicking the button. This   only outputs resources that are new or changed since the last run.</li> <li>Automatically scheduled incremental runs, as specified by   <code>incrementalSchedule</code> parameter. You can see when   the next scheduled run is near the top of the control panel.</li> </ul> <p>Note: The bottom area of the control panel shows the current options being   used by the Pipelines Controller.</p> <p></p> <p>After running the pipeline, look for the Parquet files created in the directory specified by <code>dwhRootPrefix</code> in the application.yaml file.</p>"},{"location":"installation_controller/#explore-the-configuration-settings","title":"Explore the configuration settings","text":"<p>The parameters described in this section correspond to the settings in the application.yaml file. Please reference this file for comprehensive information on each configuration parameter</p>"},{"location":"installation_controller/#fhir-search-input-parameters","title":"FHIR Search Input Parameters","text":"<ul> <li><code>fhirServerUrl</code>: Base URL of the source FHIR server. If <code>dbConfig</code> is not set,   resources are fetched from this URL through the FHIR Search API. e.g.   <code>http://172.17.0.1:8091/fhir</code></li> <li><code>fhirServerUserName</code>, <code>fhirServerPassword</code>: The user-name/password should be   set if the FHIR server supports Basic Auth. Default: <code>admin</code>, <code>Admin123</code></li> </ul>"},{"location":"installation_controller/#authentication","title":"Authentication","text":"<p>The following client credentials should be set if the source FHIR server accepts OAuth access tokens. Note the client credentials (the secret) are sensitive, and it is probably a better practice to set these through command-line arguments here.</p> <ul> <li><code>fhirServerOAuthTokenEndpoint</code></li> <li><code>fhirServerOAuthClientId</code></li> <li><code>fhirServerOAuthClientSecret</code></li> </ul>"},{"location":"installation_controller/#jdbc-input-parameters","title":"JDBC Input Parameters","text":"<ul> <li> <p><code>dbConfig</code>: The path to the file containing JDBC settings for connecting to a   HAPI FHIR server database. If this is set, resources are fetched directly from   the database and <code>fhirServerUrl</code> is ignored. Equivalent to pipeline   <code>fhirDatabaseConfigPath</code> parameter.</p> </li> <li> <p><code>dwhRootPrefix</code>: The path to output Parquet files to. The last portion of the   path is used as a prefix for naming the directory that contains per-resource   directories with an added timestamp.</p> </li> </ul> <p>Info</p> <p>Visit the application.yaml file for more file path configs based on different file systems</p>"},{"location":"installation_controller/#advanced-configurations","title":"Advanced Configurations","text":"<ul> <li><code>incrementalSchedule</code>: The schedule for automatic incremental pipeline runs.   Uses the Spring CronExpression format, i.e., \"second minute hour   day-of-the-month month day-of-the-week\"</li> <li>\"0 0 * * * *\" means top of every hour</li> <li>\"_/40 _ * * * *\" means every 40 seconds</li> </ul> <p>Info</p> <p>Scheduling very frequent runs is resource intensive</p> <ul> <li> <p><code>purgeSchedule</code>: The schedule for automatic DWH snapshot purging. There is no   benefit to scheduling the purge job more frequently than incremental runs.   Uses the Spring CronExpression format.</p> </li> <li> <p><code>numOfDwhSnapshotsToRetain</code>: The number of DWH snapshots to retain when the   purge job runs. This parameter must be &gt; 0 or the purge job will not run.   If a pipeline run fails for any reason, any partial output must be manually   removed.</p> </li> <li> <p><code>resourceList</code>: The comma-separated list of FHIR resources to fetch/monitor.   Equivalent to pipeline <code>resourceList</code> parameter. Note there is no   Questionnaire in our test FHIR server, read more   here. Default:   <code>Patient, Encounter, Observation, Questionnaire, Condition, Practitioner, Location, Organization</code></p> </li> <li> <p><code>numThreads</code>: The parallelism to be used for a pipeline job. In case of   FlinkRunner, if the value is set to -1, then in the local execution mode the   number of threads the job uses will be equal to the number of cores in the   machine, whereas in the remote mode (cluster) only 1 thread is used. If set to   a positive value, then in both modes the pipeline will use that many threads   combined across all the workers.</p> </li> <li> <p><code>autoGenerateFlinkConfiguration</code>: In case of Flink local execution mode (which   is the default), generate Flink configuration file <code>flink-conf.yaml</code>   automatically based on the parallelism set via the parameter <code>numThreads</code> and   the <code>cores</code> available in the machine. The generated file will have the   parameters set with optimised values necessary to run the pipelines without   fail. Disable this parameter to manually pass the configuration file by   pointing the environment variable FLINK_CONF_DIR to the directory where the   flink-conf.yaml is placed.</p> </li> </ul> <p>Info</p> <p>For Flink non-local execution mode, this parameter has to be disabled and the configuration file has to be passed manually which has more fine-grained control parameters</p> <ul> <li> <p><code>createHiveResourceTables</code>: Boolean determining if resource tables should be   automatically created on a Hive/Spark server. Primarily meant for   single-machine deployment.</p> </li> <li> <p><code>thriftserverHiveConfig</code>: Path to a file with the settings used to create   tables. Required if createHiveResourceTables is <code>true</code>.</p> </li> <li> <p><code>hiveResourceViewsDir</code>: Path to a directory containing   View Definition for each resource   type. If not set or set to empty string, automatic view creation is disabled.   Otherwise, for each resource type, its view definition SQL queries are read   and applied from corresponding files, i.e., any file that starts with the   resource name and ends in <code>.sql</code>, e.g., <code>DiagnosticReport_flat.sql</code>. Only   applies when createHiveResourceTables is <code>true</code>.</p> </li> </ul> <p>Developers Note:</p> <p>If you symlink <code>[repo_root]/docker/config/views</code> here you can use those predefined views in your dev environment too</p> <ul> <li> <p><code>viewDefinitionsDir</code>: The location from which   View Definition resources are   read and applied to the corresponding input FHIR resources. Any file in this   directory that ends in <code>.json</code> is assumed to be a single ViewDefinition. To   output these views to a relational database, the next <code>sinkDbConfigPath</code>   should also be set.</p> </li> <li> <p><code>structureDefinitionsPath</code>: Directory path containing the structure definition   files for any custom profiles that need to be supported. If this starts with   <code>classpath:</code> then a classpath resource is assumed and the path should always   start with a <code>/</code>. Do not configure anything if custom profiles are not needed.   Examples can be found below:</p> </li> <li> <p>\"config/r4-us-core-definitions\"</p> </li> <li> <p>\"classpath:/r4-us-core-definitions\"</p> </li> <li> <p><code>fhirVersion</code>: # The fhir version to be used for the FHIR Context APIs. This   is an enum value and the possible values should be one from the list mentioned   here.   Currently, only enums <code>R4</code> and <code>DSTU3</code> are supported by the application.</p> </li> <li> <p><code>rowGroupSizeForParquetFiles</code>: This is the size of the Parquet Row Group (a   logical horizontal partitioning into rows) that will be used for creating row   groups in parquet file by pipelines. A large value means more data for one   column can be fit into one big column chunk which will speed up the reading of   column data. On the downside, more in-memory will be needed to hold the data   before writing to files.</p> </li> <li> <p><code>recursiveDepth</code>: The maximum depth for traversing StructureDefinitions in   Parquet schema generation (if it is non-positive, the default 1 will be used).   Note in most cases, the default 1 is sufficient and increasing that can result   in significantly larger schema and more complexity. For details see   here</p> </li> </ul>"},{"location":"installation_controller/#fhir-output-parameters","title":"FHIR Output Parameters","text":"<ul> <li> <p><code>sinkDbConfigPath</code>: The configuration file for the sink database. If   <code>viewDefinitionsDir</code> is set then the generated views are materialized and   written to this DB. If not, the raw FHIR JSON resources are written to this   DB. The default empty string disables this feature. Note enabling this feature   can have a noticeable impact on pipelines performance.</p> </li> <li> <p><code>sinkFhirServerUrl</code>: The base URL of the sink FHIR server. If not set, the   feature for sending resources to a sink FHIR server is disabled.</p> </li> <li> <p><code>sinkUserName</code>, <code>sinkPassword</code>: The following user-name/password should be set   if the sink FHIR server supports Basic Auth. Default: <code>hapi</code>, <code>hapi123</code></p> </li> </ul>"},{"location":"installation_docker/","title":"FHIR Data Pipes Docker Image","text":"<p>A \u201cSingle Machine\u201d with Released Image Docker Compose Configuration  is maintained.</p> <p>This docker-compose configuration is for bringing up a pipeline controller along with a single-process Spark environment with a JDBC endpoint. This is particularly useful for quick evaluation and demo environments.</p> <pre><code>* If local paths are used, they should start with `./ `or `../`.\n* The mounted files should be readable by containers, e.g., world-readable\n</code></pre> <p>Using the \"Single Machine\" configuration you will be able to quickly and easily:</p> <ul> <li>Bring up the FHIR Pipelines Controller plus a Spark Thrift Server</li> <li>Start the Web Control Panel for interacting with the Controller for running   full and incremental Pipelines and registering new views</li> <li>Generate Parquet files for each FHIR Resource in the /docker/dwh directory</li> <li>Run the SQL-on-FHIR queries defined in /config/views/*.sql and register these   via the Thrift Server</li> <li>Generate flattened views (defined in /config/views/*.json) in the database   configured by sinkDbConfigPath (you will need to ensure the database is   created)</li> </ul> <p>Tip</p> <p>The easiest way to get up and running quickly is to follow the Single Machine Deployment tutorial.</p>"},{"location":"installation_docker/#sample-docker-configurations","title":"Sample Docker Configurations","text":"<p>The repository includes a number of other sample \"Single Machine\" docker compose configurations. These provide samples of different deployment configurations.</p> Name Description Notes Basic \"Single Machine\" Docker Compose Configuration Brings up the FHIR Pipelines Controller plus a Spark Thrift server, letting you more easily run Spark SQL queries on the Parquet files output by the Pipelines Controller. In this configuration the Spark master, one worker, and the Thrift server all run in the same container Good for getting familiar with the controller and pipelines. Will build the image from source if it does not exist Single Machine \"Local Cluster\" Docker Compose Configuration While all Spark pieces are brought up on the same machine, this config can serve as an example of how to deploy Spark on a cluster environment as it uses different containers for the master, a worker and Thrift server More complete configuration which shows different pieces that are needed for a cluster environment. See the readme for more details"},{"location":"installation_pipeline/","title":"Deploy FHIR Data Pipes ETL Pipelines","text":"<p>Guide Overview</p> <ul> <li>This guide will walk you through how to deploy and run the ETL Pipelines Java JAR without the use of the controller</li> <li>This section is intended for developers and gives an advanced overview of the ETL Pipelines functionality and its configurations</li> <li>For ease of deployment a set of example Docker Compose configurations that include both the Pipelines and Controller has been provided. See the Docker section</li> </ul>"},{"location":"installation_pipeline/#intro-to-the-etl-pipeline","title":"Intro to the ETL Pipeline","text":"<p>The ETL Pipelines is a Java JAR - designed to run on an Apache Beam - that transforms data from a FHIR source (via FHIR API, JDBC or ndjson) to either Apache Parquet files for analysis or another FHIR store for data integration. The source code is available in the <code>pipelines/batch</code> directory.</p> <p>Input or source options: There are three options for reading the source FHIR data:</p> <ul> <li>FHIR-Search: This mode uses FHIR Search APIs to select resources to copy,   retrieves them as FHIR resources, and transfers the data via FHIR APIs or   Parquet files. This mode should work with most FHIR servers and has been   tested with HAPI FHIR server and GCP FHIR store.</li> <li>JDBC: This mode uses the   Java Database Connectivity (JDBC) API   to read FHIR resources directly from the database of a FHIR server. It's   tested with   HAPI FHIR server using PostgreSQL database   or an OpenMRS instance using MySQL.</li> <li>ndjson: Newline Delimited JSON, a file format typically created from the   FHIR Bulk Export API. Reading FHIR Data in this format is not currently   supported.</li> </ul> <p>Note: JDBC support beyond HAPI FHIR and OpenMRS is not currently planned. Our long-term approach for a generic high-throughput alternative is to use the FHIR Bulk Export API.</p> <p>Output options: There are two options for transforming the data:</p> <ul> <li>Parquet: Outputs the FHIR resources as Parquet files, using the   SQL-on-FHIR schema.</li> <li>FHIR: Copies the FHIR resources to another FHIR server using FHIR APIs.</li> </ul>"},{"location":"installation_pipeline/#setup","title":"Setup","text":"<ol> <li>Clone    the FHIR Data Pipes project to    your machine.</li> <li>Set the <code>utils</code> directory to world-readable: <code>chmod -R 755 ./utils</code>.</li> <li>Build binaries by running <code>mvn clean install</code> from the root directory of the    repository.</li> </ol>"},{"location":"installation_pipeline/#run-the-pipeline","title":"Run the pipeline","text":"<p>Run the pipeline directly using the <code>java</code> command:</p> <pre><code>java -jar ./pipelines/batch/target/batch-bundled.jar \\\n    com.google.fhir.analytics.FhirEtl \\\n    --fhirServerUrl=http://example.org/fhir \\\n    --outputParquetPath=/tmp/parquet/\n    --[see additional parameters below]\n</code></pre> <p>Add the necessary parameters depending on your use case. The methods used for reading the source FHIR server and outputting the data depend on the parameters used. You can output to both Parquet files and a FHIR server by including the required parameters for both.</p>"},{"location":"installation_pipeline/#parameters","title":"Parameters","text":"<p>This section documents the parameters used by the various pipelines. For more information on parameters, see <code>FhirEtlOptions</code> or run the pipeline with the <code>help</code> option:</p> <p><code>java -jar ./batch/target/batch-bundled.jar --help=FhirEtlOptions</code>.</p>"},{"location":"installation_pipeline/#common-parameters","title":"Common parameters","text":"<p>These parameters are used regardless of other pipeline options.</p> <ul> <li><code>resourceList</code> - A comma-separated list of   FHIR resources to include in the   pipeline. Default: <code>Patient, Encounter, Observation</code></li> <li><code>runner</code> -   The Apache Beam Runner   to use. Pipelines supports <code>DirectRunner</code> and <code>FlinkRunner</code> by default. Other   runners can be enabled by Maven profiles, e.g.,   DataflowRunner.   See also   A note about Beam runners.   Default: <code>DirectRunner</code></li> </ul>"},{"location":"installation_pipeline/#fhir-search-input-parameters","title":"FHIR-Search input parameters","text":"<p>The pipeline will use FHIR-Search to fetch data as long as <code>jdbcModeEnabled</code> and <code>jdbcModeHapi</code> are false OR unset.</p> <ul> <li><code>fhirServerUrl</code> - The base URL of the source FHIR server. Required.</li> <li><code>fhirServerUserName</code> - The HTTP Basic Auth username to access the FHIR server   APIs. Default: <code>admin</code></li> <li><code>fhirServerPassword</code> - The HTTP Basic Auth password to access the FHIR server   APIs. Default: <code>Admin123</code></li> <li><code>batchSize</code> - The number of resources to fetch in each API call. Default:   <code>100</code></li> </ul>"},{"location":"installation_pipeline/#jdbc-input-parameters","title":"JDBC input parameters","text":"<p>JDBC mode is used if a JDBC flag is <code>true</code>.</p> <p>To use JDBC mode:</p> <p>1: Create a copy of hapi-postgres-config.json and edit the values to match your database server.</p> <p>2: Enable JDBC mode for your source server:</p> <ul> <li>OpenMRS</li> <li><code>jdbcModeEnabled=true</code></li> <li>HAPI FHIR server</li> <li><code>jdbcModeHapi=true</code></li> </ul> <p>3: Specify the path to your config file.</p> <ul> <li><code>fhirDatabaseConfigPath=./path/to/config.json</code></li> </ul> <p>All JDBC parameters:</p> <ul> <li><code>jdbcModeHapi</code> - If true, uses JDBC mode for HAPI FHIR server. Default:   <code>false</code></li> <li><code>jdbcModeEnabled</code> - If true, uses JDBC mode for OpenMRS. Default: <code>false</code></li> <li><code>fhirDatabaseConfigPath</code> - Path to the FHIR database config for JDBC mode.   Default: <code>../utils/hapi-postgres-config.json</code></li> <li><code>jdbcFetchSize</code> - The fetch size of each JDBC database query. Default: <code>10000</code></li> <li><code>jdbcMaxPoolSize</code> - The maximum number of database connections. Default: <code>50</code></li> </ul>"},{"location":"installation_pipeline/#parquet-output-parameters","title":"Parquet output parameters","text":"<p>Parquet files are output when <code>outputParquetPath</code> is set.</p> <ul> <li><code>outputParquetPath</code> - The file path to write Parquet files to, e.g.   <code>./tmp/parquet/</code>. Default: empty string, which does not output Parquet files.</li> <li><code>secondsToFlushParquetFiles</code> - The number of seconds to wait before flushing   all Parquet writers with non-empty content to files. Use <code>0</code> to disable.   Default: <code>3600</code>.</li> <li><code>rowGroupSizeForParquetFiles</code> - The approximate size in bytes of the   row-groups in Parquet files. When this size is reached, the content is flushed   to disk. This is not used if there are less than 100 records. Use <code>0</code> to use   the default Parquet row-group size. Default: <code>0</code>.</li> </ul>"},{"location":"installation_pipeline/#fhir-output-parameters","title":"FHIR output parameters","text":"<p>Resources will be copied to the FHIR server specified in <code>fhirSinkPath</code> if that field is set.</p> <ul> <li><code>fhirSinkPath</code> - A base URL to a target FHIR server, or the relative path of a   GCP FHIR store, e.g. <code>http://localhost:8091/fhir</code> for a FHIR server or   <code>projects/PROJECT/locations/LOCATION/datasets/DATASET/fhirStores/FHIR-STORE-NAME</code>   for a GCP FHIR store. If using a GCP FHIR store,   see this tutorial for setup information.   default: none, resources are not copied</li> <li><code>sinkUserName</code> - The HTTP Basic Auth username to access the FHIR sink. Not   used for GCP FHIR stores.</li> <li><code>sinkPassword</code> - The HTTP Basic Auth password to access the FHIR sink. Not   used for GCP FHIR stores.</li> </ul>"},{"location":"installation_pipeline/#a-note-about-beam-runners","title":"A note about Beam runners","text":"<p>If the pipeline is run on a single machine (i.e., not on a distributed cluster), for large datasets consider using a production grade runner like Flink. This can be done by adding the parameter <code>--runner=FlinkRunner</code> (use <code>--maxParallelism</code> and <code>--parallelism</code> to control parallelism). This may avoid some memory issues of <code>DirectRunner</code>.</p>"},{"location":"installation_pipeline/#example-configurations","title":"Example configurations","text":"<p>These examples are set up to work with local test servers.</p> FHIR Server to Parquet filesHAPI FHIR JDBC to a FHIR server <pre><code>java -cp ./pipelines/batch/target/batch-bundled.jar \\\n    com.google.fhir.analytics.FhirEtl \\\n    --fhirServerUrl=http://localhost:8091/fhir \\\n    --outputParquetPath=/tmp/TEST/ \\\n    --resourceList=Patient,Encounter,Observation\n</code></pre> <pre><code>java -cp ./pipelines/batch/target/batch-bundled.jar \\\n    com.google.fhir.analytics.FhirEtl \\\n    --fhirServerUrl=http://localhost:8091/fhir \\\n    --resourceList=Patient,Encounter,Observation \\\n    --fhirDatabaseConfigPath=./utils/hapi-postgres-config.json \\\n    --jdbcModeEnabled=true --jdbcModeHapi=true \\\n    --jdbcMaxPoolSize=50 --jdbcFetchSize=1000 \\\n    --jdbcDriverClass=org.postgresql.Driver \\\n    --fhirSinkPath=http://localhost:8099/fhir \\\n    --sinkUserName=hapi --sinkPassword=hapi\n</code></pre>"},{"location":"installation_pipeline/#managing-and-scheduling-the-pipeline","title":"Managing and scheduling the Pipeline","text":"<p>The ETL Pipeline is designed as a stand-alone Apache Beam service. The Pipeline Controller module provides capabilities to help manage the Pipeline including scheduling full versus incremental runs. It also provides some monitoring capabilities.</p>"},{"location":"installation_pipeline/#how-to-query-the-data-warehouse","title":"How to query the data warehouse","text":"<p>To query Parquet files, load them into a compatible data engine such as Apache Spark or use python in a jupyter notebook.</p> <p>The \"Single Machine\" docker compose configuration runs the pipeline and loads data into an Apache Spark Thrift Server for you. You can connect to this Thrift Server with a database client of you choice to query the transformed FHIR Data.</p>"},{"location":"release_process/","title":"Semantic versioning","text":"<p>Versioning across all Open Health Stack components is based on the major.minor.patch scheme and respects Semantic Versioning.</p> <p>Respecting Semantic Versioning is important for multiple reasons:</p> <ul> <li>It guarantees simple minor version upgrades, as long as you only use the   public APIs</li> <li>A new major version is an opportunity to thoroughly document breaking changes</li> <li>A new major/minor version is an opportunity to communicate new features   through a blog post</li> </ul>"},{"location":"release_process/#major-versions","title":"Major versions","text":"<p>The major version number is incremented on every breaking change.</p> <p>Whenever a new major version is released, we publish:</p> <ul> <li>a blog post with feature highlights, major bug fixes, breaking changes, and   upgrade instructions.</li> <li>an exhaustive changelog entry via the release notes</li> </ul>"},{"location":"release_process/#minor-versions","title":"Minor versions","text":"<p>The minor version number is incremented on every significant retro-compatible change.</p> <p>Whenever a new minor version is released, we publish:</p> <ul> <li>an exhaustive changelog entry via the release notes</li> </ul>"},{"location":"release_process/#patch-versions","title":"Patch versions","text":"<p>The patch version number is incremented on bugfixes releases.</p> <p>Whenever a new patch version is released, we publish:</p> <ul> <li>an exhaustive changelog entry</li> </ul>"},{"location":"support/","title":"Support","text":"<p>On this page we've listed some ways you can get technical support and Open Health Stack communities and forums that you can be a part of.</p> <p>Before participating please read our code of conduct that we expect all community members to adhere too.</p>"},{"location":"support/#developer-calls","title":"Developer calls","text":"<p>There are weekly Open Health Stack developer calls that you are welcome to join.</p> <ul> <li>Calls are on Thursdays and Alternate between Android FHIR SDK and OHS server   side (FHIR Data Pipes and Info Gateway)</li> <li>See the schedule below for more details</li> <li>To be added to the calls, please email: hello-ohs[at]google.com</li> </ul> <p>Developer call schedule</p> OHS Developers Call GMT East Africa India Android FHIR SDK 10:00 UK 12:00 Nairobi 14:30 Delhi Analytics and Info Gateway 13:00 UK 15:00 Nairobi 17:30 Delhi"},{"location":"support/#discussion-forums","title":"Discussion forums","text":"<p>We are in the process of setting up a dedicated discussion forum for Open Health Stack. In the meantime, you can reach out to hello-ohs[at]google.com</p>"},{"location":"support/#stack-overflow","title":"Stack Overflow","text":"<p>Stack Overflow is a popular forum to ask code-level questions or if you're stuck with a specific error. Read through the existing questions tagged with open-health-stack or fhir-data-pipes or ask your own!</p>"},{"location":"support/#bugs-or-feature-reqeusts","title":"Bugs or Feature reqeusts","text":"<p>Before submitting a bug or filing a feature reqeust, please review the open issues on our GitHub repository.</p> <p>If your issue is there, please add a comment. Otherwise, create a new issue to file a bug or submit a new feature request.</p> <p>Please review the contributing section.</p>"},{"location":"concepts/concepts/","title":"OHS Analytics Concepts","text":"<p>The key concepts that underpin the OHS Analytics components are:</p> <ol> <li>ETL Pipelines: ETL Pipelines and Controller can be configured to    continuously transform FHIR data into an analytics friendly    \"Parquet-on-FHIR\" schema.</li> <li>Deployment approaches: The pipelines are designed to accommodate various    deployment approaches in terms of scalability; from a single machine to a    distributed cluster environments.</li> <li>View layer for query simplification: Once the data is transformed into an    analytics-friendly format, it should be queried. Multiple approaches are    provided and/or showcased to flatten FHIR schema to make developing analytics    solutions easier.</li> </ol>"},{"location":"concepts/concepts/#fhir-data-pipes","title":"FHIR Data Pipes","text":"<p>FHIR Data Pipes is built on Apache Beam SDK for ease of scalability and has multiple deployment options from local, to on-prem clusters to public clouds.</p> <p>FHIR Data Pipes is made up of the ETL Pipelines and Controller modules * *that are designed to work together** to provide continuous transformation of FHIR data to Apache Parquet files (for data analysis) or another FHIR server ( for data integration).</p> <p></p>"},{"location":"concepts/concepts/#etl-pipelines","title":"ETL Pipelines","text":"<p>Java binaries to Extract, Transform and Load FHIR data from a FHIR source to Parquet format.</p>"},{"location":"concepts/concepts/#extraction","title":"Extraction","text":"<p>FHIR Data Pipes is designed to fetch FHIR source data in various forms and APIs:</p> <ul> <li>FHIR Search API: This should work with   most FHIR servers, including those using FHIR data model like HAPI FHIR or   FHIR facades like OpenMRS.</li> <li>Direct database access: This is mostly a more efficient approach that works   with HAPI FHIR only.</li> <li>Bulk Export API: This will gradually become our main fetch API as more FHIR   servers implement the   Bulk Export API.</li> <li>Files in json and ndjson formats.</li> </ul>"},{"location":"concepts/concepts/#transform-to-parquet-on-fhir","title":"Transform to Parquet-on-FHIR","text":"<p>FHIR Resources are transformed into a \"Parquet-on-FHIR\" format which forms the \"base DWH\".</p> <ul> <li>Uses a forked version of   Bunsen library   ( currently supports STU3 and R4 versions of FHIR)</li> <li>Configurable support for FHIR profiles and extensions</li> <li>(Optional) In-pipeline 'flattening' of FHIR data using   ViewDefinition   resources - read more</li> </ul>"},{"location":"concepts/concepts/#loading","title":"Loading","text":"<p>FHIR Data Pipes supports different SQL Data Warehouse options depending on the needs of the project. These include:</p> <ul> <li>Loading Parquet files into an OLAP query engine such as SparkSQL (which can be   deployed on a single machine or a distributed cluster) or DuckDB (single   machine in-process) or many other tools that understand Parquet.</li> <li>Traditional relational databases such as PostgreSQL (when using FHIR   ViewDefinition   resources to generate materialized views)</li> </ul>"},{"location":"concepts/concepts/#pipeline-controller-module","title":"Pipeline Controller Module","text":"<p>A user-interface wrapper for the FHIR Data Pipes Pipelines, integrating \" full\", \"incremental\", and \"merger\" pipelines together.</p> <ul> <li>The Pipelines Controller is built on top of pipelines and shares many of the   same settings</li> <li>Using the controller module you can schedule periodic incremental updates or   use the Web Control Panel to start the   pipeline manually</li> </ul>"},{"location":"concepts/concepts/#deployment-approaches","title":"Deployment Approaches","text":"<p>There are a number of different deployment approaches - see table below.</p> <p>Choosing the right approach, comes down to a number of factors including the specific use-case, infrastructure constraints of the project, future scalability requirements, and expertise of the team.</p> Scenario Approach Considerations Simple relational database to power dashboards or reporting Custom schema defined as ViewDefinition Resources - see Views By design, this will provide a subset of FHIR data in flat tables Horizontally scalable query architecture with lossless FHIR data Parquet based DWH and distributed query engine (e.g. SparkSQL) - see tutorial This requires a distributed file system for Parquet files and a distributed query engine. Lossless FHIR DWH with a single process query engine Parquet based DWH with non-distributed query engine set-up (e.g. single-node Spark or DuckDB) Scalability and performance (e.g. SparkSQL &gt;&gt; DuckDB) Exploratory data science or ML use cases Use the generated Parquet files which as \"near lossless\" for enhanced data science workflows Can either use the Parquet or custom schema to power dashboards or reports Push FHIR data to a central FHIR-store (e.g., for a Shared Health Record system) Use the Pipelines Controller to push from a FHIR source to a FHIR sink Management of the intermediate Parquet files created as part of the pipelines"},{"location":"concepts/concepts/#view-layer-for-simplifying-queries","title":"View layer for simplifying queries","text":"<p>FHIR Data Pipes provides two approaches for flattening the FHIR resources into virtual or materialized views:</p> <ol> <li> <p>SQL queries to generate virtual views (outside the pipeline)</p> </li> <li> <p>FHIR    ViewDefinition    resources to generate materialized views (within the pipeline)</p> </li> </ol> <p>For more information on both of these approaches, please check Schema and Flat Views.</p>"},{"location":"concepts/views/","title":"Schema and Flat Views","text":""},{"location":"concepts/views/#overview","title":"Overview","text":"<p>The heavily nested nature of FHIR resources and the Parquet-on-FHIR schema requires complex SQL queries that can make them difficult to work with for analytics use cases. A common approach to address this is to flatten the data into a set of views (virtual or materialized) which can then be queried using simpler SQL statements.</p>"},{"location":"concepts/views/#parquet-on-fhir-schema","title":"Parquet-on-FHIR schema","text":"<p>The FHIR Data Pipes ETL Pipelines convert raw FHIR resources to a Parquet-on-FHIR schema representation. This takes place for each resource type and follows the Schema Mapping Rules.</p> <p>The generated columnar Parquet files provide the \"base data warehouse\" that can be queried using any Parquet-aware tools (e.g a SQL based query engine) or further transformed via the view layer into materialized views.</p>"},{"location":"concepts/views/#flattening-via-the-view-layer","title":"Flattening via the view layer","text":"<p>FHIR Data Pipes provides two approaches for flattening the FHIR resources into virtual or materialized views:</p> <ol> <li> <p>SQL queries to generate    virtual views    ( outside the pipeline)</p> </li> <li> <p>FHIR    ViewDefinition    resources to generate materialized views (within the pipeline)</p> </li> </ol> <p>For both of these approaches, a set of \"predefined views\" for common FHIR resources are provided. These can be modified or extended.</p> <p>The currently supported list (as of July, 2024) are:</p> <pre><code>Condition\nDiagnosticReport\nEncounter\nImmunization\nLocation\nMedicationrequest\nObservation\nOrganization\nPatient\nPractitioner\nPractitionerRole\nProcedure\n</code></pre>"},{"location":"concepts/views/#sql-virtual-views","title":"SQL virtual views","text":"<p>These are samples of more complex SQL-on-FHIR queries for defining flat views for common FHIR resources. These virtual views are applied outside the pipelines in a downstream SQL query engine.</p> <p>The queries, which have <code>.sql</code> suffix, can be found in /docker/config/views directory (e.g <code>Patient_flat.sql</code>).</p> <p>An example of a flat view for the Observation resource is below:</p> <pre><code>CREATE OR REPLACE VIEW flat_observation AS\nSELECT O.id AS obs_id, O.subject.PatientId AS patient_id,\n        OCC.`system` AS code_sys, OCC.code,\n        O.value.quantity.value AS val_quantity,\n        OVCC.code AS val_code, OVCC.`system` AS val_sys,\n        O.effective.dateTime AS obs_date\n      FROM Observation AS O LATERAL VIEW OUTER explode(code.coding) AS OCC\n        LATERAL VIEW OUTER explode(O.value.codeableConcept.coding) AS OVCC\n</code></pre>"},{"location":"concepts/views/#query-simplification","title":"Query Simplification","text":"<p>The following example is taken from a tutorial Jupyter notebook available here.</p> <p>The following queries count the number of patients that have had an observation with a specific code (HIV viral load), with a value below a certain threshold for the year 2010.</p> Standalone QueryQuery with Views <p><pre><code>SELECT COUNT(DISTINCT O.subject.PatientId) AS num_patients\n  FROM Observation AS O LATERAL VIEW explode(code.coding) AS OCC\n  WHERE OCC.code LIKE '856%%'\n    AND OCC.`system` = 'http://loinc.org'\n    AND O.value.quantity.value &lt; 400000\n    AND YEAR(O.effective.dateTime) = 2010;\n</code></pre> The output relation should have a count of 3074 patients: <pre><code>+---------------+\n| num_patients  |\n+---------------+\n| 3074          |\n+---------------+\n</code></pre></p> <p><pre><code>SELECT COUNT(DISTINCT patient_id) AS num_patients\n  FROM Observation_flat\n  WHERE code LIKE '856%%'\n    AND code_sys = 'http://loinc.org'\n    AND val_quantity &lt; 400000\n    AND YEAR(obs_date) = 2010\n  LIMIT 100;\n</code></pre> The output relation should have a count of 3074 patients: <pre><code>+---------------+\n| num_patients  |\n+---------------+\n| 3074          |\n+---------------+\n</code></pre></p> <p>This approach preserves the nested structures and arrays of FHIR resources within the <code>Observation_flat</code> view. The results of these queries can then be used as arbitrary tables for further data analysis in other tools.</p>"},{"location":"concepts/views/#viewdefinition-resource","title":"ViewDefinition resource","text":"<p>The SQL-on-FHIR-v2 specification defines a ViewDefinition resource for defining views. Each column in the view is defined using a FHIRPath expression. There is also an un-nesting construct and support for <code>constant</code> and <code>where</code> clauses too.</p> <p>Note</p> <ul> <li>A singlular View Definition will not join different resources in any way</li> <li>Each View Definition defines a tabular view of exactly one resource type</li> </ul> <p>A system (pipeline or library) that implements the \u201cView Layer\u201d of the specification provides a View Runner that is able to process these FHIR ViewDefinition Resources over the \u201cData Layer\u201d (lossless representation of the FHIR data). The output of this are a set of portable, tabular views that can be consumed by the \u201cAnalytics Layer\u201d which is any number of tools that can be used to work with the resulting tabular data.</p> <p>FHIR Data Pipes is a reference implementation of the SQL-on-FHIR-v2 specification:</p> <ul> <li> <p>The \"View Runner\" is, by default, part of the ETL Pipelines and uses the   transformed Parquet files as the \u201cData Layer\u201d. This can be extracted to be a   stand-alone component if required</p> </li> <li> <p>When enabled as part of the Pipeline configuration, thr \"View Runner\" will   apply the ViewDefinition resources from the   views folder   and materialize the resulting tables to the configured database (an instance   of PostgresSQL, MySQL, etc.).</p> </li> <li> <p>A set of pre-defined ViewDefinitions for common FHIR resources is provided as   part of the default package. These can be adapted, replaced and extended.</p> </li> <li> <p>The FHIR Data Pipes provides a simple ViewDefinition Editor which can be used   to explore FHIR ViewDefinitions and apply these to individual FHIR resources.</p> </li> </ul> <p>Once the FHIR data has been transformed via the ETL Pipelines, the resulting schema is available for querying using a JDBC interface.</p> <p>Visit our interactive playground to get a hands-on understanding of the Patient ViewDefinition resource, and many more</p>"},{"location":"concepts/views/#viewdefinition-editor","title":"ViewDefinition editor","text":"<p>The ViewDefinition editor provides a way to quickly evaluate ViewDefinition resources against sample FHIR data. You access it as part of the Web Control Panel, selecting the \"Views\" navigation item in the top right corner.</p> <p>Using the ViewDefinition editor you can:</p> <ul> <li>Provide an input ViewDefinition (left)</li> <li>Apply it to a sample input FHIR resource (right pane)</li> <li>View the results in the generated table (top)</li> </ul> <p></p>"},{"location":"concepts/views/#output-data-formats","title":"Output Data Formats","text":"<p>Applying the FHIR ViewDefinition resources to the \"base dwh\" will generate materialized views which represent a \"constrained\" set of data to be used for downstream analytics applications (such as dashboards or reporting). This feature is enabled when the viewDefinitionsDir is set.</p> <p>These can be outputted in any tabular format with current support for Database tables and Parquet files.</p>"},{"location":"concepts/views/#conversion-to-database-tables","title":"Conversion to Database tables","text":"<p>The resulting database tables can be loaded into a commonly used relational database management system such as PostgresSQL or MySQL. This is enabled when the sinkDbConfigPath is set.</p>"},{"location":"concepts/views/#conversion-to-parquet","title":"Conversion to Parquet","text":"<p>The resulting Parquet files can be easily loaded into any Parquet-aware query engine. Examples include SparkSQL or duckdb</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>In this section you can access tutorials to learn about different aspects of working with FHIR Data Pipes.</p> <ul> <li>Single Machine Deployment: Great for getting started   quickly and evaluating FHIR Data Pipes</li> <li>Visualize Parquet DWH with Apache Superset: With the   Pipelines set-up, learn how to visualize the data using Apache Superset</li> <li>Set up local test servers: Set up local HAPI FHIR server   and load it with synthetic data</li> </ul>"},{"location":"tutorials/add_dashboard/","title":"Visualize Parquet DWH with Apache Superset","text":"<p>Generating dashboards is a common task for many digital health projects.</p> <p>This tutorial shows you how to use the popular open source Apache Superset package to build a dashboard on top of the \"lossless\" parquet DWH created via the FHIR Data Pipes Single Machine Deployment Tutorial.</p> <p>The principles here will apply to any visualization or BI tool that has a hive connector.</p>"},{"location":"tutorials/add_dashboard/#prerequisites","title":"Prerequisites","text":"<ul> <li>A FHIR Data Pipes single machine deployment using   local test servers as the data source</li> <li>Run the full pipeline at least once before following this guide.</li> </ul>"},{"location":"tutorials/add_dashboard/#install-apache-superset","title":"Install Apache Superset","text":"<p>There are several ways to install Superset. A simple option is to install Superset locally using docker-compose. Once it is set-up, log in by visiting http://localhost:8088/ and using the default credentials username: <code>admin</code> password: <code>admin</code>.</p>"},{"location":"tutorials/add_dashboard/#connect-to-the-apache-spark-sql-data-source","title":"Connect to the Apache Spark SQL data source","text":"<ol> <li>In the upper-right corner, select Settings &gt; Database Connections.</li> <li>In the upper-right corner, select + Database.</li> </ol>"},{"location":"tutorials/add_dashboard/#create-the-connection-for-spark-sql","title":"Create the connection for Spark SQL","text":"<ol> <li>Under Supported Databases, select Apache Spark SQL.</li> <li>Under Display Name, enter \"Single Machine Test Data\" or another name your    choice.</li> <li>Under SQLAlchemy URI, enter the following:</li> </ol> <pre><code>hive://hive@&lt;IP_address_of_docker_network_gateway&gt;:10001\n</code></pre> <p>To find the IP address, run:</p> <pre><code>docker network inspect bridge --format='{{json .IPAM.Config}}'\n</code></pre> <ol> <li>Select Test Connection. You should see a pop-up in the bottom-right    corner that says \"Connection looks good!\"    - If you get an error, double-check the IP address is correct. You may also      need to install the Apache Spark SQL database drivers.</li> <li>Select Connect. Although you see an error: \"An error occurred while    creating databases: Fatal error\" the connection is still created.</li> <li>Close the Connect a database window by clicking outside of it and refresh    the Databases page. The database you just created should appear.</li> </ol>"},{"location":"tutorials/add_dashboard/#create-a-dashboard","title":"Create a dashboard","text":"<p>Create an empty dashboard so you can add charts to it as they are created.</p> <ol> <li>At the top of the Superset page select the Dashboard tab, then click +    Dashboard.</li> <li>Select the title area which reads [ untitled dashboard ] and give the    dashboard a name. The examples below use \"Sample Dashboard\".</li> <li>In the upper-right corner, select Save.</li> </ol>"},{"location":"tutorials/add_dashboard/#query-the-database-and-generate-charts","title":"Query the database and generate Charts","text":"<ol> <li>From the top tabs, select SQL &gt; SQL Lab.</li> <li>If this is your first time using SQL Lab, you should be in a blank, empty    tab. If not, click New tab or press <code>Control+T</code> to create one.</li> <li>On the left side under Database, select the database you created earlier,    for example Single Machine Test Data.</li> </ol>"},{"location":"tutorials/add_dashboard/#add-a-big-number","title":"Add a Big Number","text":"<p>This first chart shows the total number of patients as a \"Big Number\".</p> <ol> <li>Replace the placeholder query <code>SELECT ...</code> with the following:</li> </ol> <pre><code>SELECT COUNT(*) FROM Patient;\n</code></pre> <ol> <li>Click Run. After a moment, the results of the query should appear.</li> <li>In the Results section, click Create Chart. This brings you to a new    chart page.</li> <li>Under the Chart Source pane, click Create a dataset and name it    \"Patient Count Dataset\".</li> <li>Under the Data tab, switch to the 4k (Big Number) chart type.</li> <li>In the Query section, click the Metric section and set the following    values:</li> <li>Column: <code>count(1)</code></li> <li>Aggregate: <code>MAX</code></li> <li>Click Update Chart at the bottom of the pane.</li> </ol> <p>The chart updates to show a single large number of the count of patients.</p> <ol> <li>Click Save to bring up the Save chart dialog. Name the chart \"Patient    Count\" and under Add to Dashboard select the dashboard you created    previously.</li> </ol>"},{"location":"tutorials/add_dashboard/#add-a-pie-chart","title":"Add a pie chart","text":"<p>The next chart shows the split of patients by gender as a pie chart.</p> <ol> <li>Navigate back to SQL Lab. You should see the previous patient count query.    - If you'd like to keep the patient count query, make a new tab and set the      database to \"Single Machine Test Data\" or the name you chose.</li> <li>Enter the following as the query:</li> </ol> <pre><code>SELECT `gender` AS `gender`,\n       count(`gender`) AS `COUNT(gender)`\nFROM Patient AS P\nGROUP BY `gender`\nORDER BY `COUNT(gender)` DESC;\n</code></pre> <ol> <li>Click Run. After a moment, the results of the query should appear.</li> <li>In the Results section, click Create Chart. This brings you to a new    chart page.</li> <li>Under the Chart Source pane, click Create a dataset and name it    \"Gender Split Dataset\".</li> <li>Under the Data tab, switch to the Pie Chart chart type.</li> <li>In the Query section, set the following values:    - Dimensions:<ol> <li>Column: <code>gender</code>    - Metric: 2. Column: <code>COUNT(gender)</code> 3. Aggregate: <code>MAX</code></li> </ol> </li> <li>Click Update Chart at the bottom of the pane. The chart updates to show a    pie chart of patients by gender.</li> <li>Click Save to bring up the Save chart dialog. Name the chart \"Gender    Split\" and under Add to Dashboard select the dashboard you created    previously.</li> </ol>"},{"location":"tutorials/add_dashboard/#add-a-complex-bar-chart","title":"Add a complex bar chart","text":"<p>The next chart shows number of patients on HIV treatment by year and display that as a bar chart over time. The query is provided below.</p> <ul> <li>Navigate back to SQL Lab.</li> <li>You should see the previous patient count query.</li> <li>If you'd like to keep the patient count query, make a new tab and set the   database to \"Single Machine Test Data\" or the name you chose.</li> </ul> <p>Enter the following as the query as either a raw SQL-on-FHIR query or using the pre-defined observation_flat views:</p> Using SQL-on-FHIR QueryWith pre-defined flat views <pre><code>SELECT COUNT(*), YEAR(O.effective.dateTime)\nFROM Observation AS O LATERAL VIEW EXPLODE(code.coding) AS OCC LATERAL VIEW EXPLODE(O.value.codeableConcept.coding) AS OVCC\nWHERE OCC.code LIKE '1255%'\nAND OVCC.code LIKE \"1256%\"\nAND YEAR(O.effective.dateTime) &lt; 2023\nGROUP BY YEAR(O.effective.dateTime)\nORDER BY YEAR(O.effective.dateTime) ASC\n</code></pre> <pre><code>SELECT COUNT(*), YEAR(o.obs_date)\nFROM observation_flat as o\nWHERE o.code LIKE '1255%'\n    AND o.val_code LIKE \"1256%\"\n    AND YEAR(o.obs_date) &lt; 2023\nGROUP BY YEAR(o.obs_date)\nORDER BY YEAR(o.obs_date) ASC\n</code></pre> <p><code>The codes 1255% and 1266% refer to HIV\\_Tx codes in the dataset</code></p> <ul> <li>Click Run. After a moment, the results of the query should appear.</li> <li>In the Results section, click Create Chart. This brings you to a new   chart page.</li> <li>Under the Chart Source pane, click Create a dataset and name it \"HIV   Treatment Dataset\".</li> <li>Under the Data tab, switch to the Bar Chart chart type.</li> <li>In the Query section, set the following values:</li> </ul> <p>X-Axis: Custom SQL: <code>YEAR(date\\_time)</code></p> <p>Metric: Column: <code>patient\\_id</code>, Aggregate: <code>COUNT</code></p> <ul> <li>Click Update Chart at the bottom of the pane. The chart updates to show a   bar chart of patients undergoing HIV treatment per year.</li> <li>Click Save to bring up the Save chart dialog. Name the chart \"HIV   Treatment\" and under Add to Dashboard select the dashboard you created   previously.</li> </ul>"},{"location":"tutorials/add_dashboard/#edit-the-dashboard","title":"Edit the Dashboard","text":"<p>Navigate to the Dashboards section and select the Sample Dashboard you created. You should see something like this with the charts you created.</p> <p></p> <p>Create a dashboard with 2 rows and a header. First click Edit Dashboard then:</p> <ol> <li>In the right pane select the Layout Elements tab.</li> <li>Add a second row by dragging a Row element onto the canvas below the    existing row.</li> <li>Arrange the charts to have the Patient Count and Gender Split in the top row,    and HIV Treatment in the bottom row.</li> <li>Resize the top charts to use half of the width by dragging the edge of the    chart.</li> <li>Resize the HIV Treatment chart to take up the whole width of the row.</li> <li>Add a header element at the top and enter \"Key Program Metrics\".</li> </ol> <p>Your dashboard now looks something like this:</p> <p></p>"},{"location":"tutorials/add_dashboard/#update-the-dashboard","title":"Update the dashboard","text":"<p>To check that the dashboard is working properly, add a new patient to the FHIR Store and run the incremental pipeline.</p> <ol> <li>Add a new patient to the server:</li> </ol> <pre><code>curl -X POST -H \"Content-Type: application/fhir+json; charset=utf-8\" \\\n     'http://localhost:8091/fhir/Patient/' -d '{\"resourceType\": \"Patient\"}'\n</code></pre> <ol> <li>Alternatively, use the HAPI FHIR server tester; go to the Patient resource    tester, click the CRUD Operations tab, paste the following resource    into the Contents field of the Create section, then click Create.</li> </ol> <pre><code>{\"resourceType\": \"Patient\"}\n</code></pre> <ol> <li>Open the Pipeline Controller at http://localhost:8090 and run the incremental    pipeline.</li> <li>Once the pipeline has finished, go to Superset and open your dashboard. Click    the ... button next to Edit Dashboard and then click Refresh    dashboard. You should see the patient count increase by 1.</li> </ol>"},{"location":"tutorials/add_dashboard/#learn-more","title":"Learn more","text":"<p>For more information about Superset, go to the Superset documentation.</p>"},{"location":"tutorials/gcp_fhirstore/","title":"Create a Google Cloud FHIR Store and BigQuery Dataset","text":"<p>One of the supported FHIR sinks is Google Cloud FHIR store and BigQuery. These instructions will guide you in creating a FHIR store and BigQuery dataset.</p> <p>To set up GCP project that you can use as a sink FHIR store:</p> <ul> <li>Create a new project in GCP. For an   overview of projects, datasets and data stores check   this document.</li> <li>Enable Google Cloud Healthcare API in the project and create a Google Cloud   Healthcare dataset</li> <li>Create a FHIR data store in the dataset with the R4 FHIR version</li> <li>Enable the BigQuery API in the project and dataset with the same name in the   project</li> <li>Download, install, and initialize the <code>gcloud</code> cli:   https://cloud.google.com/sdk/docs/quickstart</li> <li>Make sure you can authenticate with the project using the CLI:   https://developers.google.com/identity/sign-in/web/sign-in</li> <li><code>$ gcloud init</code></li> <li><code>$ gcloud auth application-default login</code></li> <li>Create a service account for the project, generate a key, and save it     securely locally</li> <li>Add the <code>bigquery.dataEditor</code> and <code>bigquery.jobUser</code> roles to the project in     the <code>IAM &amp; Admin</code>/<code>Roles</code> settings or using the cli:<ul> <li><code>$ gcloud projects add-iam-policy-binding openmrs-260803 --role roles/bigquery.admin --member serviceAccount:openmrs-fhir-analytics@openmrs-260803.iam.gserviceaccount.com</code></li> <li><code>$ gcloud projects add-iam-policy-binding openmrs-260803 --role roles/healthcare.datasetAdmin --member serviceAccount:openmrs-fhir-analytics@openmrs-260803.iam.gserviceaccount.com</code></li> </ul> </li> <li>Activate the service account for your project using     <code>gcloud auth activate-service-account &lt;your-service-account&gt; --key-file=&lt;your-key-file&gt; --project=&lt;your project&gt;</code></li> <li>Set the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable:     https://cloud.google.com/docs/authentication/getting-started#setting_the_environment_variable</li> <li> <p>Use the script <code>utils/create_fhir_store.sh</code> to   create a FHIR store in this dataset which stream the changes to the BigQuery   dataset as well:   <code>./utils/create_fhir_store.sh PROJECT LOCATION DATASET FHIR-STORE-NAME</code></p> </li> <li> <p><code>PROJECT</code> is your GCP project.</p> </li> <li><code>LOCATION</code> is GCP location where your dataset resides, e.g., <code>us-central1</code>.</li> <li><code>DATASET</code> is the name of the dataset you created.</li> <li><code>FHIR-STORE-NAME</code> is what it says.</li> </ul> <p>Note: If you get <code>PERMISSION_DENIED</code> errors, make sure to   <code>IAM &amp; ADMIN</code>/<code>IAM</code>/<code>Members</code> and add the <code>bigquery.dataEditor</code> and   <code>bigquery.jobUser</code> roles to the <code>Cloud Healthcare Service Agent</code> service   account that shows up.</p> <p>You can run the script with no arguments to see a sample usage. After you create the FHIR store, its full URL would be:</p> <p><code>https://healthcare.googleapis.com/v1/projects/PROJECT/locations/LOCATION/datasets/DATASET/fhirStores/FHIR-STORE-NAME</code></p>"},{"location":"tutorials/single_machine/","title":"Single machine deployment tutorial","text":"<p>The repository includes a \"Single Machine\" Docker Compose configuration which brings up the FHIR Pipelines Controller plus a Spark Thrift server, letting you more easily run Spark SQL queries on the Parquet files output by the Pipelines Controller.</p> <p>To learn how the Pipelines Controller works on its own, try out the FHIR Pipelines Controller.</p>"},{"location":"tutorials/single_machine/#requirements","title":"Requirements","text":"<ul> <li>A source HAPI FHIR server configured to   use Postgres as its database</li> <li>If you don't have a server, use a     local test server     by following the instructions to bring up a source HAPI FHIR server with     Postgres</li> <li>Docker</li> <li>If you are using Linux, Docker must be in     sudoless mode</li> <li>Docker Compose - this guide assumes you   are using the latest version</li> <li>The FHIR Data Pipes repository, cloned onto the host machine</li> </ul>"},{"location":"tutorials/single_machine/#configure-the-fhir-pipelines-controller","title":"Configure the FHIR Pipelines Controller","text":"<p>All file paths are relative to the root directory of the FHIR Data Pipes repository.</p> <p>NOTE: You need to configure only one of the following options:</p> For FHIR Search API (works for any FHIR server):For direct DB access (specific to HAPI FHIR servers): <ol> <li>Open <code>docker/config/application.yaml</code> and edit the parameter <code>fhirServerUrl</code> to match the FHIR server you are connecting to.</li> <li>Comment out the paramter: <code>dbConfig</code>.</li> </ol> <ol> <li>Open <code>docker/config/application.yaml</code> and comment out the paramter: <code>fhirServerUrl</code></li> <li>Set <code>dbConfig</code> to the DB connection config file, e.g. <code>docker/config/hapi-postgres-config_local.json</code>.</li> <li>Edit the json values in this file to match the database for the FHIR server you are connecting to.</li> </ol>"},{"location":"tutorials/single_machine/#parquet-files-and-flattened-views","title":"Parquet Files and Flattened Views","text":"<p>FHIR Data Pipes supports the option to transform FHIR data into an RDBMS (as the data-warehouse) using FHIR View Definition resources to define the custom schema. This option in our tutorial will be specific to PostgreSQL, but this feature is compatible with any RDBMS.</p> <p>The concept of \"flattened views\" comes from applying ViewDefinitions to FHIR Resources to get flat tables. Please check Schema and Flat Views to learn more.</p> <p>With the default config, you will create both Parquet files ( under <code>dwhRootPrefix</code>) and flattened views in the relational database configured by <code>sinkDbConfigPath</code> here (The <code>sinkDbConfigPath</code> refers to the target DB that will become the data-warehouse)</p> <p>If you do not want flat tables in a sink DB, comment out that parameter!</p> <p>If you do need them, make sure you create the DB referenced in the connection config file with the following SQL query:</p> <pre><code>CREATE DATABASE views;\n</code></pre> <p>Run this query in Postgres:</p> <pre><code>PGPASSWORD=admin psql -h 127.0.0.1 -p 5432 -U admin postgres -c \"CREATE DATABASE views\"\n</code></pre> <p>For documentation of all config parameters, see here.</p> <p>Note: If you are using the local test servers, things should work with the default values. If not, use the IP address of the Docker default bridge network. To find it, run the following command and use the \"Gateway\" value:</p> <pre><code>docker network inspect bridge | grep Gateway\n</code></pre> <p></p> <p>The Single Machine docker configuration uses two environment variables, <code>DWH_ROOT</code> and <code>PIPELINE_CONFIG</code>, whose default values are defined in the .env file.</p> <p>To override them: set the desired variable before running the <code>docker-compose</code> command. For example, to override the <code>DWH_ROOT</code> environment variable, run the following:</p> <pre><code>DWH_ROOT=\"$(pwd)/&lt;path_to_dwh_directory&gt;\" docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate\n</code></pre>"},{"location":"tutorials/single_machine/#run-the-single-machine-configuration","title":"Run the Single Machine Configuration","text":"<p>To bring up the <code>docker/compose-controller-spark-sql-single.yaml</code> configuration for the first time or if you have run this container in the past and want to include new changes pulled into the repo, run:</p> <pre><code>docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate --build\n</code></pre> <p>Alternatively, to run without rebuilding use:</p> <pre><code>docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate\n</code></pre> <p>Alternatively, <code>docker/compose-controller-spark-sql.yaml</code> serves as a very simple example on how to integrate the Parquet output of Pipelines in a Spark cluster environment.</p> <p>Once started, the Pipelines Controller is available at <code>http://localhost:8090</code> and the Spark Thrift server is at <code>http://localhost:10001</code>.</p> <p>The first time you run the Pipelines Controller, you are required to start a Full Pipeline run. In a browser go to <code>http://localhost:8090</code> and click the Run Full button.</p> <p>After running the Full Pipeline, use the Incremental Pipeline to update the Parquet files and tables. By default, it is scheduled to run every hour, or you can manually trigger it.</p> <p></p> <p>If the Incremental Pipeline does not work, or you see errors like:</p> <pre><code>ERROR o.openmrs.analytics.PipelineManager o.openmrs.analytics.PipelineManager$PipelineThread.run:343 - exception while running pipeline:\npipeline-controller    | java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n</code></pre> <p>Try running <code>sudo chmod -R 755</code> on the Parquet file directory, by default located at <code>docker/dwh</code>.</p>"},{"location":"tutorials/single_machine/#explore-the-resulting-schema-in-postgresql","title":"Explore the resulting schema in PostgreSQL","text":"<p>If you set the <code>sinkDbConfigPath</code> parameter to output views to a PostgreSQL DWH, connect to the DB via docker using this cmd: <code>docker exec -it &lt;container_name_or_id&gt; bash</code></p> <p>If using the default container (hapi-fhir-db) run: <code>docker exec -it hapi_fhir_db bash</code></p> <p>Using psql connect to the 'views' database: <code>psql -U admin -d views</code></p> <p>To list the tables: <code>\\d</code>. It should look something like this:</p> Schema Name Type Owner public condition_flat table admin public diagnostic_report_flat table admin public immunization_flat table admin public location_flat table admin public medication_request_flat table admin public observation_flat table admin public organization_flat table admin public practitioner_flat table admin public practitioner_role_flat table admin public procedure_flat table admin"},{"location":"tutorials/single_machine/#view-and-analyze-the-data","title":"View and Analyze the Data","text":"<p>Let's do some basic quality checks to make sure the data is uploaded properly (note table names are case-insensitive).</p> Spark Thrift ServerSink Database <p>Connect to the Spark Thrift server using a client that supports Apache Hive. For example, if using the JDBC driver, the URL should be <code>jdbc:hive2://localhost:10001</code>.</p> <p>The pipeline will automatically create <code>Patient</code>, <code>Encounter</code>, and <code>Observation</code> tables when run.</p> <p><pre><code>SELECT COUNT(0) FROM Patient;\n</code></pre> We should have exactly 79 patients: <pre><code>+-----------+\n| count(0)  |\n+-----------+\n| 79        |\n+-----------+\n</code></pre></p> <p>Doing the same for observations: <pre><code>SELECT COUNT(0) FROM Observation;\n</code></pre> <pre><code>+-----------+\n| count(0)  |\n+-----------+\n| 17279     |\n+-----------+\n</code></pre></p> <p>Connect to your sink DB with the database client of your choice.</p> <p>Note: You will see that the number of patients and observations is higher than the count in the Thrift Server. This is due to the flattening</p> <p><pre><code>SELECT COUNT(0) FROM patient_flat;\n</code></pre> We should have exactly 114 patients: <pre><code>+-----------+\n| count     |\n+-----------+\n| 114       |\n+-----------+\n</code></pre></p> <p>Let's do the same for observations: <pre><code>SELECT COUNT(0) FROM observation_flat;\n</code></pre> <pre><code>+-----------+\n| count  |\n+-----------+\n| 18343     |\n+-----------+\n</code></pre></p>"},{"location":"tutorials/single_machine/#whats-next","title":"What's Next","text":"<p>Now that the data is available in an SQL queryable format, you can start to explore it using SQL or jupyter notebooks or build dashboards using common open source tools like Apache SuperSet.</p>"},{"location":"tutorials/test_servers/","title":"Set-up Local Test Servers","text":"<p>This guide shows how to use provided Docker images to bring up test servers (and optionally load with synthetic data) to easily get started the FHIR Data Pipes pipelines.</p> <p>There are 3 Docker server configurations you can use for testing:</p> <ul> <li>HAPI FHIR server with Postgres (source)</li> <li>OpenMRS Reference Application with MySQL (source)</li> <li>HAPI FHIR server (destination)</li> </ul>"},{"location":"tutorials/test_servers/#instructions","title":"Instructions","text":"<p>Note: All commands are run from the root directory of the repository.</p> <ol> <li> <p>Create an external Docker network named <code>cloudbuild</code>:</p> <pre><code>docker network create cloudbuild\n</code></pre> </li> <li> <p>Bring up a FHIR source server for the pipeline. This can be either:</p> <ul> <li>HAPI FHIR server with Postgres:</li> </ul> <pre><code>docker-compose  -f ./docker/hapi-compose.yml up  --force-recreate -d\n</code></pre> <p>The base FHIR URL for this server is <code>http://localhost:8091/fhir</code>. If you   get a CORS error when accessing the URL, try manually refreshing (e.g.   ctrl-shift-r).</p> <ul> <li>OpenMRS Reference Application with MySQL:</li> </ul> <pre><code>docker-compose -f ./docker/openmrs-compose.yml up --force-recreate -d\n</code></pre> <p>The base FHIR URL for this server is   <code>http://localhost:8099/openmrs/ws/fhir2/R4</code></p> </li> <li> <p>Upload the synthetic data stored in     sample_data     to the FHIR server that you brought up using the     Synthea data uploader.</p> <p>The uploader requires the <code>google-auth</code> Python library, which you can install using:</p> <pre><code>pip3 install --upgrade google-auth\n</code></pre> <p>For example, to upload to the HAPI FHIR server brought up in the previous step, run:</p> <pre><code>python3 ./synthea-hiv/uploader/main.py HAPI http://localhost:8091/fhir \\\n--input_dir ./synthea-hiv/sample_data --cores 8\n</code></pre> <p>Depending on your machine, using too many cores may slow down your machine or cause JDBC connection pool errors with the HAPI FHIR server. Reducing the number of cores using the <code>--cores</code> flag should help at the cost of increasing the time to upload the data.</p> </li> <li> <p>(optional) If you only want to output Apache Parquet files, there is no     additional setup. If you want to test outputting to another FHIR server,     then bring up a destination     HAPI FHIR server:</p> <pre><code>docker-compose  -f ./docker/sink-compose.yml up  --force-recreate -d\n</code></pre> <p>The base URL for this server is <code>http://localhost:8098/fhir</code>.</p> </li> </ol>"},{"location":"tutorials/test_servers/#additional-notes-for-openmrs","title":"Additional notes for OpenMRS","text":"<p>Once running you can access OpenMRS at http://localhost:8099/openmrs/ using username \"admin\" and password \"Admin123\". The Docker image includes the required FHIR2 module and demo data. Edit <code>docker/openmrs-compose.yaml</code> to change the default port.</p> <p>Note: If <code>docker-compose</code> fails, you may need to adjust file permissions. In particular if the permissions on <code>mysqld.cnf</code> is not right, the <code>datadir</code> set in this file will not be read by MySQL and it will cause OpenMRS to require its <code>initialsetup</code> (which is not needed since the MySQL image already has all the data and tables needed):</p> <pre><code>$ docker-compose -f docker/openmrs-compose.yaml down -v\n$ chmod a+r docker/mysql-build/mysqld.cnf\n$ chmod -R a+r ./utils\n$ docker-compose -f docker/openmrs-compose.yaml up\n</code></pre> <p>In order to see the demo data in OpenMRS you must rebuild the search index. In OpenMRS go to Home &gt; System Administration &gt; Advanced Administration. Under Maintenance go to Search Index then Rebuild Search Index.</p>"},{"location":"tutorials/try_controller/","title":"Try the Pipelines Controller with HAPI FHIR Store","text":"<p>The FHIR Pipelines Controller makes it easy to schedule and manage the transformation of data from a HAPI FHIR server to a collection of Apache Parquet files. It uses FHIR Data Pipes JDBC pipeline to run either full or incremental transformations to a Parquet data warehouse.</p> <p>The FHIR Pipelines Controller only works with HAPI FHIR servers using Postgres. You can see an example of configuring a HAPI FHIR server to use Postgres here.</p> <p>This guide will show you how to set up the FHIR Pipelines Controller with a test HAPI FHIR server. It assumes you are using Linux, but should work with other environments with minor adjustments.</p>"},{"location":"tutorials/try_controller/#clone-the-fhir-data-pipes-repository","title":"Clone the fhir-data-pipes repository","text":"<p>Clone the fhir-data-pipes GitHub repository using your preferred method. After cloned, open a terminal window and <code>cd</code> to the directory where you cloned it. Later terminal commands will assume your working directory is the repository root.</p>"},{"location":"tutorials/try_controller/#set-up-the-test-server","title":"Set up the test server","text":"<p>The repository includes a Docker Compose configuration to bring up a HAPI FHIR server configured to use Postgres.</p> <p>To set up the test server, follow these instructions. At step two, follow the instructions for \"HAPI source server with Postgres\".</p>"},{"location":"tutorials/try_controller/#configure-the-fhir-pipelines-controller","title":"Configure the FHIR Pipelines Controller","text":"<p>First, open <code>pipelines/controller/config/application.yml</code> in a text editor.</p> <p>Change fhirServerUrl to be:</p> <pre><code>fhirServerUrl: \"http://localhost:8091/fhir\"\n</code></pre> <p>Read through the rest of the file to see other settings. The other lines may remain the same. Note the value of <code>dwhRootPrefix</code>, as it will be where the Parquet files are written. You can also adjust this value if desired. Save and close the file.</p> <p>Next, open <code>pipelines/controller/config/hapi-postgres-config.json</code> in a text editor.</p> <p>Change <code>databaseHostName</code> to be:</p> <pre><code>\"databaseHostName\" : \"localhost\"\n</code></pre> <p>Save and close the file.</p>"},{"location":"tutorials/try_controller/#run-the-fhir-pipelines-controller","title":"Run the FHIR Pipelines Controller","text":"<p>From the terminal run:</p> <pre><code>cd pipelines/controller/\nmvn spring-boot:run\n</code></pre> <p>Open a web browser and visit http://localhost:8080. You should see the FHIR Pipelines Control Panel.</p> <p></p> <p>Before automatic incremental runs can occur, you must manually trigger a full run. Under the Run Full Pipeline section, click on Run Full. Wait for the run to complete.</p>"},{"location":"tutorials/try_controller/#explore-the-configuration-settings","title":"Explore the configuration settings","text":"<p>The Control Panel shows the options being used by the FHIR Pipelines Controller.</p>"},{"location":"tutorials/try_controller/#main-configuration-parameters","title":"Main configuration parameters","text":"<p>This section corresponds to the settings in the <code>application.yml</code> file.</p>"},{"location":"tutorials/try_controller/#batch-pipeline-non-default-configurations","title":"Batch pipeline non-default configurations","text":"<p>This section calls out FHIR Data Pipes batch pipeline settings that are different from their default values. These are also mostly derived from <code>application.yml</code>. Use these settings if you want to run the batch pipeline manually.</p>"},{"location":"tutorials/try_controller/#query-the-dwh","title":"Query the DWH","text":"<p>On your machine, look for the Parquet files created in the directory specified by <code>dwhRootPrefix</code> in the application.yml file. You can explore the data using a Jupyter notebook with pyspark and pandas libraries installed</p> <p>Alternatively you can load the Parquet into an SQL Query Engine like SparkSQL. See the tutorials section for more.</p>"}]}