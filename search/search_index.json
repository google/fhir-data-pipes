{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The Open Health Stack's Analytics components provide a scalable and flexible collection of tools to transform complex HL7 FHIR data into formats for running analytics workloads and building downstream applications.</p> <p>Using OHS, developers can use familiar languages and tools to build analytics solutions for different use cases: from generating reports and powering dashboards to exploratory data science and machine learning.</p> <p></p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li> <p>Apache Beam based ETL pipelines to continuously   transform FHIR   resources to \"near lossless\" 'FHIR-in-Parquet' representation based on a   natural schema   for projecting FHIR resource to Parquet.</p> </li> <li> <p>Pipelines Controller module provides pipeline management and scheduling   capabilities.</p> </li> <li> <p>Flexible deployment modes to meet the needs of different projects and teams   from simple single machine to multi-worker horizontally scalable distributed   environments. With support for local, on-prem or cloud based runners.</p> </li> <li> <p>Support for different target databases including traditional RDBMS (such   as PostgreSQL) or any OLAP engines that can   load Parquet files (such as SparkSQL   or DuckDB).</p> </li> <li> <p>Simplify querying data by defining views in SQL or   as ViewDefinition   resources to create flattened tables. Easily build analytics applications with   common languages (e.g. SQL, python) and BI or data visualizations tools (   e.g. Apache Superset).</p> </li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<ul> <li> <p>The primary use case for FHIR Data Pipes is to enable continuous   transformation of FHIR data into analytics friendly representations to make it   easier for developers to: build dashboards, generate reports, perform data   science task, and create features for machine learning models.</p> </li> <li> <p>A secondary use case is for piping FHIR data from a FHIR source to another   FHIR server e.g. for integration into a central FHIR repository.</p> </li> </ul>"},{"location":"additional/","title":"Additional topics","text":""},{"location":"additional/#authentication","title":"Authentication","text":"<p>Any openIDConnect provider can be used to supply oAuth credentials.</p> <p>The required set of parameters are:</p> <ul> <li>oidConnectUrl</li> <li>clientId</li> <li>clientSecret</li> <li>oAuthUsername</li> <li>oAuthPassword</li> </ul> <p>You can add oAuth authentication to the pipeline by providing the required set of parameters via the command line or in the <code>/pipelines/controller/config/application.yaml</code> file.</p> Cmd lineapplication.yaml <pre><code>$ java -cp ./pipelines/batch/target/batch-bundled-0.1.0-SNAPSHOT.jar org.openmrs.analytics.FhirEtl \\\n    --fhirServerUrl=[FHIR_SERVER_URL] --outputParquetPath=[PATH] \\\n    --resourceList=Patient,Encounter,Observation --batchSize=200 \\\n    --clientId=[CLIENT_ID] --clientSecret=[CLIENT_SECRET] \\\n    --OAuthUsername=[USERNAME] --OAuthPassword=[PASSSWORD] \\\n    --oidConnectUrl= [OPENID_CONNECT_URL] \\\n</code></pre> <pre><code>fhirdata:\n.....\n    # The following client credentials should be set if the FHIR server accepts\n    # OAuth access tokens. Note the client credentials, e.g., the secret, are\n    # sensitive, and it is probably a better practice to set these through\n    # command-line arguments.\n    fhirServerOAuthTokenEndpoint: \"https://path_to_endpoint_for_token\"\n    fhirServerOAuthClientId: \"THE_CLIENT_ID\"\n    fhirServerOAuthClientSecret: \"THE_CLIENT_SECRET\"\n</code></pre>"},{"location":"additional/#config-properties","title":"Config properties","text":"<p>The main configuration for the FHIR Data Pipes Pipeline and Controller is the <code>/pipelines/controller/config/application.yaml</code> file which is well documented.</p> <p>When using the provided docker images, this will be found in <code>/docker/config.application.yaml</code></p>"},{"location":"additional/#parquet-on-fhir-schema","title":"Parquet on FHIR schema","text":"<p>Apache Parquet is a horizontally scalable columnar format that is optimized for performance.</p> <p>FHIR Data Pipes transforms FHIR resources to \"near lossless\" 'Parquet on FHIR' representation based on the \"Simplified SQL Projection of FHIR Resources\" ( 'SQL-on-FHIR-v1') schema</p> <ul> <li> <p>The conversion is done using a forked version   of Bunsen library   to transform from FHIR (current support for STU3, R4) to the SQL-on-FHIR-v1   schema</p> </li> <li> <p>The conversion is done by going from StructureDefinition --&gt; AvroConverter --&gt;   Parquet</p> </li> <li> <p>Configurable support for FHIR versions, profiles and extensions is provided</p> </li> </ul>"},{"location":"additional/#monitoring-pipelines","title":"Monitoring pipelines","text":"<p>The pipelines controller exposes a number of management end-points that can help with monitoring the health of pipelines.</p> <ul> <li>The application has been integrated with the Spring Boot Actuator of Spring   and has exposed Rest API end points for monitoring, health checks, metrics   etc.</li> <li>The end points can be customised in the configuration file.</li> <li>It can easily be integrated with tools like Prometheus for monitoring metrics.</li> </ul> <p>Via the Web Control Panel The Web Control panel provides a quick glimpse about the latest state of the application including:</p> <ul> <li>Controls for triggering pipeline run on-demand</li> <li>A readable view of the application configuration</li> <li>Location and time of the latest snapshot created by the pipeline run</li> <li>Metrics of the most recent pipeline</li> <li>Error logs of the last pipeline if any</li> </ul> <p>These are found in the <code>application.yaml</code> config file in the <code>management:</code> section.</p> <p>See Config properties</p>"},{"location":"additional/#web-control-panel","title":"Web Control Panel","text":"<p>The web control panel is a basic spring application provided to make interacting with the pipeline controller easier.</p> <p>It is not designed to be a full production ready \u201cweb admin\u201d panel.</p> <p>The web control panel has the following features:</p> <ul> <li>Initiate full and incremental pipeline runs</li> <li>Monitor errors when running pipelines</li> <li>Recreate view tables</li> <li>View configuration settings</li> <li>Access sample jupyter notebooks and ViewDefinition editor</li> </ul> <p></p>"},{"location":"concepts/","title":"OHS Analytics Concepts","text":"<p>The key concepts that underpin the OHS Analytics components are:</p> <ol> <li>ETL Pipelines: ETL Pipelines and Controller can be configured to    continuously transform FHIR data into an analytics friendly FHIR-in-Parquet    format.</li> <li>Deployment approaches: The pipelines are designed to accommodate various    deployment approaches in terms of scalability; from a single machine to a    distributed cluster environments.</li> <li>Query simplification approaches: Once the data is transformed into an    analytics-friendly format, it should be queried. Multiple approaches are    provided and/or showcased to flatten FHIR schema to make developing analytics    solutions easier.</li> </ol>"},{"location":"concepts/#fhir-data-pipes","title":"FHIR Data Pipes","text":"<p>FHIR Data Pipes is built on Apache Beam SDK for ease of scalability and has multiple deployment options from local, to on-prem clusters to public clouds.</p> <p>FHIR Data Pipes is made up of the ETL Pipelines and Controller modules * that are designed to work together* to provide continuous transformation of FHIR data to Apache Parquet files (for data analysis) or another FHIR server ( for data integration).</p> <p></p>"},{"location":"concepts/#etl-pipelines","title":"ETL Pipelines","text":"<p>Java binaries to Extract, Transform and Load FHIR data from a FHIR source to Parquet format.</p>"},{"location":"concepts/#extraction","title":"Extraction","text":"<p>FHIR Data Pipes is designed to fetch FHIR source data in various forms and APIs:</p> <ul> <li>FHIR Search API: This should work with   most FHIR   servers, including those using FHIR data model like HAPI FHIR or FHIR facades   like OpenMRS.</li> <li>Direct database access: This is mostly a more efficient approach that works   with HAPI FHIR only.</li> <li>Bulk Export API: This will gradually become our main fetch API as more FHIR   servers implement   the Bulk Export API.</li> <li>Files in json and ndjson formats.</li> </ul>"},{"location":"concepts/#transformation","title":"Transformation","text":"<p>FHIR Resources are transformed into a \"Parquet on FHIR\" format:</p> <ul> <li>Uses a forked version   of Bunsen library (_   currently supports STU3 and R4 versions of FHIR).</li> <li>Configurable support for FHIR profiles and extensions</li> <li>(Optional) In-pipeline 'flattening' of FHIR data   using ViewDefinition   resources - read more</li> </ul>"},{"location":"concepts/#loading","title":"Loading","text":"<p>FHIR Data Pipes supports different SQL Data Warehouse options depending on the needs of the project. These include:</p> <ul> <li>Loading Parquet files into an OLAP query engine such as SparkSQL (which can be   deployed on a single machine or a distributed cluster) or DuckDB (single   machine in-process) or many other tools that understand Parquet.</li> <li>Traditional relational databases such as PostgreSQL (when using   FHIR ViewDefinition   resources to generate materialized views)</li> </ul>"},{"location":"concepts/#pipeline-controller-module","title":"Pipeline Controller Module","text":"<p>A user-interface wrapper for the FHIR Data Pipes Pipelines, integrating \" full\", \"incremental\", and \"merger\" pipelines together.</p> <ul> <li>The Pipelines Controller is built on top of pipelines and shares many of the   same settings</li> <li>Using the controller module you can schedule periodic incremental updates or   use a web control panel to start the pipeline   manually</li> </ul>"},{"location":"concepts/#deployment-approaches","title":"Deployment approaches","text":"<p>There are a number of different deployment approaches - see table below.</p> <p>Choosing the right approach, comes down to a number of factors including the specific use-case, infrastructure constraints of the project, future scalability requirements, and expertise of the team.</p> Scenario Approach Considerations Simple relational database to power dashboards or reporting Custom schema defined as ViewDefinition Resources - see tutorial By design, this will provide a subset of FHIR data in flat tables Horizontally scalable query architecture with lossless FHIR data Parquet based DWH and distributed query engine (e.g. SparkSQL) - see tutorial This requires a distributed file system for Parquet files and a distributed query engine. Lossless FHIR DWH with a single process query engine Parquet based DWH with non-distributed query engine set-up (e.g. single-node Spark or DuckDB) Scalability and performance (e.g. SparkSQL &gt;&gt; DuckDB) Exploratory data science or ML use cases Use the generated Parquet files which as \"near lossless\" for enhanced data science workflows Can either use the Parquet or custom schema to power dashboards or reports Push FHIR data to a central FHIR-store (e.g., for a Shared Health Record system) Use the Pipelines Controller to push from a FHIR source to a FHIR sink Management of the intermediate Parquet files created as part of the pipelines"},{"location":"concepts/#query-simplification-approaches-with-pre-defined-views","title":"Query simplification approaches with pre-defined views","text":"<p>The heavily nested nature of FHIR resources requires complex SQL queries that can make it difficult to work with for analytics use cases. A common approach to address this is to flatten the data into a set of views (virtual or materialized) which can then be queried using simpler SQL statements.</p> <p>FHIR Data Pipes provides two approaches for flattening the FHIR resources into virtual or materialized views:</p> <ol> <li>SQL queries to generate virtual views (outside the pipeline) 2. FHIR ViewDefinition resources to generate materialized views (within the pipeline)</li> </ol> <p>For both of these approaches, a set of **\"predefined views\u201d ** for common resources are provided. These can be modified or extended.</p> <p>The currently supported list (as of June, 2024) are:</p> <pre><code>Condition\nDiagnosticReport\nEncounter\nImmunization\nLocation\nMedicationrequest\nObservation\nOrganization\nPatient\nPractitioner\nPractitionerRole\nProcedure\n</code></pre>"},{"location":"concepts/#sql-virtual-views","title":"SQL virtual views","text":"<p>These are samples of more complex SQL-on-FHIR queries for defining flat views for common FHIR resources. These virtual views are applied outside of the pipelines in the downstream SQL query engine.</p> <p>The queries, which have <code>.sql</code> suffix, can be found in /docker/config/views directory (e.g <code>Patient_flat.sql</code>).</p> <p>An example of a flat view for the Observation resource is below:</p> <pre><code>CREATE OR REPLACE VIEW flat_observation AS\nSELECT O.id AS obs_id, O.subject.PatientId AS patient_id,\n        OCC.`system` AS code_sys, OCC.code,\n        O.value.quantity.value AS val_quantity,\n        OVCC.code AS val_code, OVCC.`system` AS val_sys,\n        O.effective.dateTime AS obs_date\n      FROM Observation AS O LATERAL VIEW OUTER explode(code.coding) AS OCC\n        LATERAL VIEW OUTER explode(O.value.codeableConcept.coding) AS OVCC\n</code></pre>"},{"location":"concepts/#viewdefinition-resource","title":"ViewDefinition resource","text":"<p>The SQL-on-FHIR-v2 specification defines a ViewDefinition resource for defining views. Each column in the view is defined using a FHIRPath expression. There is also an unnesting construct and support for <code>constant</code> and <code>where</code> clauses too.</p> <p>A system (pipeline or library) that implements the \u201cView Layer\u201d of the specification provides a View Runner that is able to process these FHIR ViewDefinition Resources over the \u201cData Layer\u201d (lossless representation of the FHIR data). The output of this are a set of portable, tabular views that can be consumed by the \u201cAnalytics Layer\u201d which is any number of tools that can be used to work with the resulting tabular data.</p> <p>FHIR Data Pipes is a reference implementation of the SQL-on-FHIR-v2 specification:</p> <ul> <li> <p>The \"View Runner\" is by default part of the ETL Pipelines and uses the   transformed Parquet files as the \u201cData Layer\u201d. This can be extracted to be a   stand-alone component if required</p> </li> <li> <p>When enabled as part of the Pipeline configuration, it will apply the   ViewDefinition resources from   the views folder   and materialize the resulting tables to the configured database (e.g., an   instance of PostgresSQL, MySQL, etc.).</p> </li> <li> <p>A set of pre-defined ViewDefinitions for common FHIR resources is provided as   part of the default package. These can be adapted, replaced and extended.</p> </li> <li> <p>The FHIR Data Pipes provides a simple ViewDefinition Editor which can be used   to explore FHIR ViewDefinitions and apply these to individual FHIR resources.</p> </li> </ul> <p>Once the FHIR data has been transformed via the ETL Pipelines, the resulting schema is available for querying using a JDBC interface.</p>"},{"location":"concepts/#viewdefinition-editor","title":"ViewDefinition editor","text":"<p>The ViewDefinition editor provides a way to quickly evaluate ViewDefinition resources against sample FHIR data. You access it as part of the Web Control Panel, selecting the \"Views\" navigation item in the top right corner.</p> <p>Using the ViewDefinition editor you can:</p> <ul> <li>Provide an input ViewDefinition (left)</li> <li>Apply it to a sample input FHIR resource (right pane)</li> <li>View the results in the generated table (top)</li> </ul> <p></p>"},{"location":"contributing/","title":"How to Contribute","text":"<p>We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.</p>"},{"location":"contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>Contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one.</p> <p>You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.</p>"},{"location":"contributing/#code-reviews","title":"Code reviews","text":"<p>All submissions by non-project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. We use GitHub for issue tracking.</p>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<p>This project follows Google's Open Source Community Guidelines.</p>"},{"location":"getting_started/","title":"Start Here","text":"<p>FHIR Data Pipes can be installed and deployed on a number of different platforms and using different architectural patterns depending on the environment, amount of data and requirements for horizontal scalability.</p> <p>Installation options include:</p> <ul> <li>Via docker images</li> <li>Deploy Data Pipes \"Pipelines\"</li> <li>Deploy Data Pipes \"Pipelines Controller\"</li> </ul> <p>Quick Start Guide</p> <p>The easiest way to get started is to follow the Single Machine Deployment tutorial. This uses a set of docker images and provided sample data to bring up an end-to-end environment for getting started with FHIR Data Pipes on a single machine</p>"},{"location":"installation_controller/","title":"Deploy the Controller Module","text":"<p>Guide Overview</p> <ul> <li>This guide will walk you through how to deploy and run the Pipelines Controller Module</li> <li>This module is designed to work with the FHIR Data Pipes ETL Pipelines</li> <li>For ease of deployment a set of example docker compose configurations that include the Pipeline and Controller has been provided. See the Docker section</li> </ul>"},{"location":"installation_controller/#usage","title":"Usage","text":"<p>The FHIR Pipelines Controller helps you schedule and manage the running of the FHIR Data Pipes Pipeline. Using the Controller module you can configure the Pipeline to run either full or incremental transformations, and it can be used to monitor the pipelines.</p> <p>Once deployed, the Controller can be managed via the cmd-line or Web Control Panel.</p>"},{"location":"installation_controller/#setup","title":"Setup","text":"<ol> <li>Clone the    fhir-data-pipes GitHub repository,    open a terminal window.</li> <li><code>cd</code> to the directory where you cloned it.</li> <li>Change to the <code>controller</code> directory: <code>cd pipelines/controller/</code>.</li> </ol> <p>Later terminal commands will assume your working directory is the <code>controller</code> directory.</p> <p>Next, configure the FHIR Pipelines Controller. The FHIR Pipelines Controller relies on several configuration files to run. Edit them to match your environment and requirements.</p> <ul> <li>Main settings for Pipeline Controller   : <code>pipelines/controller/config/application.yaml</code>.   Edit the values to match your HAPI FHIR server and use case.</li> <li>JDBC settings   : <code>pipelines/controller/config/hapi-postgres-config.json</code>.   Edit the values to match the Postgres database of your HAPI FHIR server.</li> <li>Customized FlinkRunner settings   : <code>pipelines/controller/config/flink-conf.yaml</code>.   To use this file, set   the <code>FLINK_CONF_DIR</code> environmental variable:   <code>export FLINK_CONF_DIR=[PATH]/flink-conf.yaml</code>. Changing   <code>taskmanager.memory.network.max</code> is necessary to avoid memory errors for   large datasets or when running on machines with limited memory.</li> <li>**Spark Server settings:   ** <code>pipelines/controller/config/thriftserver-hive-config.json</code>.   Used to connect to a Spark server when <code>createHiveResourceTables</code> is set   to <code>true</code>. Edit the values to match your Spark server if necessary.</li> </ul>"},{"location":"installation_controller/#run-the-fhir-pipelines-controller","title":"Run the FHIR Pipelines Controller","text":"<p>There are 2 ways to run the FHIR Pipelines Controller.</p> <p>Using Spring Boot:</p> <pre><code>mvn spring-boot:run\n</code></pre> <p>Running the JAR directly:</p> <pre><code>mvn clean install\njava -jar ./target/controller-bundled.jar\n</code></pre> <p>After running, open a web browser and visit http://localhost:8080. You should see the FHIR Pipelines Control Panel.</p> <p>There are 3 ways to have the FHIR Pipelines Controller run the transformation pipeline:</p> <ul> <li>Manually trigger the Run Full option by clicking the button. This   transforms all the selected FHIR resource types to Parquet files. You are   required to use the Run Full option once before using any of the following   incremental options.</li> <li>Manually trigger the Run Incremental option by clicking the button. This   only outputs resources that are new or changed since the last run.</li> <li>Automatically scheduled incremental runs, as specified by   <code>incrementalSchedule</code> in   the <code>[application.yaml]((https://github.com/google/fhir-data-pipes/tree/master/pipelines/controller/config/application.yaml)</code>   file. You can see when the   next scheduled run is near the top of the control panel.</li> </ul> <p>After running the pipeline, look for the Parquet files created in the directory specified by <code>dwhRootPrefix</code> in the <code>[application.yaml]((https://github.com/google/fhir-data-pipes/tree/master/pipelines/controller/config/application.yaml)</code> file.</p>"},{"location":"installation_controller/#explore-the-configuration-settings","title":"Explore the configuration settings","text":"<p>The bottom area of the control panel shows the options being used by the FHIR Pipelines Controller.</p>"},{"location":"installation_controller/#main-configuration-parameters","title":"Main configuration parameters","text":"<p>This section corresponds to the settings in the <code>[application.yaml]((https://github.com/google/fhir-data-pipes/tree/master/pipelines/controller/config/application.yaml)</code> file.</p>"},{"location":"installation_controller/#batch-pipeline-non-default-configurations","title":"Batch pipeline non-default configurations","text":"<p>This section calls out FHIR Data Pipes batch pipeline settings that are different from their default values. These are also mostly derived from <code>[application.yaml]((https://github.com/google/fhir-data-pipes/tree/master/pipelines/controller/config/application.yaml)</code>. Use these settings if you want to run the batch pipeline manually.</p>"},{"location":"installation_docker/","title":"FHIR Data Pipes Docker Image","text":"<p>A \u201cSingle Machine\u201d with Released Image Docker Compose Configuration  is maintained.</p> <p>This docker-compose configuration is for bringing up a pipeline controller along with a single-process Spark environment with a JDBC endpoint. This is particularly useful for quick evaluation and demo environments.</p> <pre><code>* If local paths are used, they should start with `./ `or `../`. \n* The mounted files should be readable by containers, e.g., world-readable\n</code></pre> <p>Using the \"Single Machine\" configuration you will be able to quickly and easily:</p> <ul> <li>Bring up the FHIR Pipelines Controller plus a Spark Thrift Server</li> <li>Start the Web Control Panel for interacting with the Controller for running   full and incremental Pipelines and registering new views</li> <li>Generate Parquet files for each FHIR Resource in the /docker/dwh directory</li> <li>Run the SQL-on-FHIR queries defined in /config/views/*.sql and register these   via the Thrift Server</li> <li>Generate flattened views (defined in /config/views/*.json) in the database   configured by sinkDbConfigPath (you will need to ensure the database is   created)</li> </ul> <p>Tip</p> <p>The easiest way to get up and running quickly is to follow the Single Machine Deployment tutorial.</p>"},{"location":"installation_docker/#sample-docker-configurations","title":"Sample Docker Configurations","text":"<p>The repository includes a number of other sample \"Single Machine\" docker compose configurations. These provide samples of different deployment configurations.</p> Name Description Notes Basic \"Single Machine\" Docker Compose Configuration Brings up the FHIR Pipelines Controller plus a Spark Thrift server, letting you more easily run Spark SQL queries on the Parquet files output by the Pipelines Controller. In this configuration the Spark master, one worker, and the Thrift server all run in the same container Good for getting familiar with the controller and pipelines. Will build the image from source if it does not exist Single Machine \"Local Cluster\" Docker Compose Configuration While all Spark pieces are brought up on the same machine, this config can serve as an example of how to deploy Spark on a cluster environment as it uses different containers for the master, a worker and Thrift server More complete configuration which shows different pieces that are needed for a cluster environment. See the readme for more details"},{"location":"installation_pipeline/","title":"Deploy FHIR Data Pipes ETL Pipelines","text":"<p>Guide Overview</p> <ul> <li>This guide will walk you through how to deploy and run the ETL Pipelines Java JAR</li> <li>To help with scheduling and managing the ETL Pipeline, a separate Pipeline Controller module is provided (see next guide)</li> <li>For ease of deployment a set of example docker compose configurations that include the Pipeline and Controller has been provided. See the Docker section</li> </ul>"},{"location":"installation_pipeline/#intro-to-the-etl-pipeline","title":"Intro to the ETL Pipeline","text":"<p>The ETL Pipelines is a Java JAR - designed to run on an Apache Beam - that transforms data from a FHIR source (via FHIR API, JDBC or ndjson) to either Apache Parquet files for analysis or another FHIR store for data integration. The source code is available in the <code>pipelines/batch</code> directory.</p> <p>Input or source options: There are three options for reading the source FHIR data:</p> <ul> <li>FHIR-Search: This mode uses FHIR Search APIs to select resources to copy,   retrieves them as FHIR resources, and transfers the data via FHIR APIs or   Parquet files. This mode should work with most FHIR servers and has been   tested with HAPI FHIR server and GCP FHIR store.</li> <li>JDBC: This mode uses the   Java Database Connectivity (JDBC) API   to read FHIR resources directly from the database of a FHIR server. It's   tested   with HAPI FHIR server using PostgreSQL database   or an OpenMRS instance using MySQL.</li> <li>ndjson: To do</li> </ul> <p>Note: JDBC support beyond HAPI FHIR and OpenMRS is not currently planned. Our long-term approach for a generic high-throughput alternative is to use the FHIR Bulk Export API.</p> <p>Output options: There are two options for transforming the data:</p> <ul> <li>Parquet: Outputs the FHIR resources as Parquet files, using the   SQL-on-FHIR schema.</li> <li>FHIR: Copies the FHIR resources to another FHIR server using FHIR APIs.</li> </ul>"},{"location":"installation_pipeline/#setup","title":"Setup","text":"<ol> <li>Clone    the FHIR Data Pipes project to    your machine.</li> <li>Set the <code>utils</code> directory to world-readable: <code>chmod -R 755 ./utils</code>.</li> <li>Build binaries by running <code>mvn clean install</code> from the root directory of the    repository.</li> </ol>"},{"location":"installation_pipeline/#run-the-pipeline","title":"Run the pipeline","text":"<p>Run the pipeline directly using the <code>java</code> command:</p> <pre><code>java -jar ./pipelines/batch/target/batch-bundled.jar \\\n    com.google.fhir.analytics.FhirEtl \\\n    --fhirServerUrl=http://example.org/fhir \\\n    --outputParquetPath=/tmp/parquet/\n    --[see additional parameters below]\n</code></pre> <p>Add the necessary parameters depending on your use case. The methods used for reading the source FHIR server and outputting the data depend on the parameters used. You can output to both Parquet files and a FHIR server by including the required parameters for both.</p>"},{"location":"installation_pipeline/#parameters","title":"Parameters","text":"<p>This section documents the parameters used by the various pipelines. For more information on parameters, see <code>FhirEtlOptions</code> or run the pipeline with the <code>help</code> option:</p> <p><code>java -jar ./batch/target/batch-bundled.jar --help=FhirEtlOptions</code>.</p>"},{"location":"installation_pipeline/#common-parameters","title":"Common parameters","text":"<p>These parameters are used regardless of other pipeline options.</p> <ul> <li><code>resourceList</code> - A comma-separated list of   FHIR resources to include in the   pipeline. Default: <code>Patient,Encounter,Observation</code></li> <li><code>runner</code> -   The Apache Beam Runner   to use. Pipelines supports <code>DirectRunner</code> and <code>FlinkRunner</code> by default; other   runners can be enabled by Maven profiles, e.g.,   DataflowRunner.   See also   A note about Beam runners.   Default: <code>DirectRunner</code></li> </ul>"},{"location":"installation_pipeline/#fhir-search-input-parameters","title":"FHIR-Search input parameters","text":"<p>The pipeline will use FHIR-Search to fetch data as long as <code>jdbcModeEnabled</code> and <code>jdbcModeHapi</code> are false OR unset.</p> <ul> <li><code>fhirServerUrl</code> - The base URL of the source FHIR server. Required.</li> <li><code>fhirServerUserName</code> - The HTTP Basic Auth username to access the FHIR server   APIs. Default: <code>admin</code></li> <li><code>fhirServerPassword</code> - The HTTP Basic Auth password to access the FHIR server   APIs. Default: <code>Admin123</code></li> <li><code>batchSize</code> - The number of resources to fetch in each API call. Default:   <code>100</code></li> </ul>"},{"location":"installation_pipeline/#jdbc-input-parameters","title":"JDBC input parameters","text":"<p>JDBC mode is used if a JDBC flag is <code>true</code>.</p> <p>To use JDBC mode:</p> <p>1: Create a copy of hapi-postgres-config.json and edit the values to match your database server.</p> <p>2: Enable JDBC mode for your source server:</p> <ul> <li>OpenMRS<ul> <li><code>jdbcModeEnabled=true</code></li> </ul> </li> <li>HAPI FHIR server<ul> <li><code>jdbcModeHapi=true</code></li> </ul> </li> </ul> <p>3: Specify the path to your config file.</p> <ul> <li><code>fhirDatabaseConfigPath=./path/to/config.json</code></li> </ul> <p>All JDBC parameters:</p> <ul> <li><code>jdbcModeHapi</code> - If true, uses JDBC mode for HAPI FHIR server. Default:   <code>false</code></li> <li><code>jdbcModeEnabled</code> - If true, uses JDBC mode for OpenMRS. Default: <code>false</code></li> <li><code>fhirDatabaseConfigPath</code> - Path to the FHIR database config for JDBC mode.   Default: <code>../utils/hapi-postgres-config.json</code></li> <li><code>jdbcFetchSize</code> - The fetch size of each JDBC database query. Default: <code>10000</code></li> <li><code>jdbcMaxPoolSize</code> - The maximum number of database connections. Default: <code>50</code></li> </ul>"},{"location":"installation_pipeline/#parquet-output-parameters","title":"Parquet output parameters","text":"<p>Parquet files are output when <code>outputParquetPath</code> is set.</p> <ul> <li><code>outputParquetPath</code> - The file path to write Parquet files to, e.g.,   <code>./tmp/parquet/</code>. Default: empty string, which does not output Parquet files.</li> <li><code>secondsToFlushParquetFiles</code> - The number of seconds to wait before flushing   all Parquet writers with non-empty content to files. Use <code>0</code> to disable.   Default: <code>3600</code>.</li> <li><code>rowGroupSizeForParquetFiles</code> - The approximate size in bytes of the   row-groups in Parquet files. When this size is reached, the content is flushed   to disk. This is not used if there are less than 100 records. Use <code>0</code> to use   the default Parquet row-group size. Default: <code>0</code>.</li> </ul>"},{"location":"installation_pipeline/#fhir-output-parameters","title":"FHIR output parameters","text":"<p>Resources will be copied to the FHIR server specified in <code>fhirSinkPath</code> if that field is set.</p> <ul> <li><code>fhirSinkPath</code> - A base URL to a target FHIR server, or the relative path of a   GCP FHIR store, e.g. <code>http://localhost:8091/fhir</code> for a FHIR server or   <code>projects/PROJECT/locations/LOCATION/datasets/DATASET/fhirStores/FHIR-STORE-NAME</code>   for a GCP FHIR store. If using a GCP FHIR store,   see this tutorial for setup information.   default: none, resources are not copied</li> <li><code>sinkUserName</code> - The HTTP Basic Auth username to access the FHIR sink. Not   used for GCP FHIR stores.</li> <li><code>sinkPassword</code> - The HTTP Basic Auth password to access the FHIR sink. Not   used for GCP FHIR stores.</li> </ul>"},{"location":"installation_pipeline/#a-note-about-beam-runners","title":"A note about Beam runners","text":"<p>If the pipeline is run on a single machine (i.e., not on a distributed cluster), for large datasets consider using a production grade runner like Flink. This can be done by adding the parameter <code>--runner=FlinkRunner</code> (use <code>--maxParallelism</code> and <code>--parallelism</code> to control parallelism). This may avoid some of the memory issues of <code>DirectRunner</code>.</p>"},{"location":"installation_pipeline/#example-configurations","title":"Example configurations","text":"<p>These examples are set up to work with local test servers.</p> FHIR Search to Parquet filesHAPI FHIR JDBC to a FHIR server <pre><code>java -cp ./pipelines/batch/target/batch-bundled.jar \\\n    com.google.fhir.analytics.FhirEtl \\\n    --fhirServerUrl=http://localhost:8091/fhir \\\n    --outputParquetPath=/tmp/TEST/ \\\n    --resourceList=Patient,Encounter,Observation\n</code></pre> <pre><code>java -cp ./pipelines/batch/target/batch-bundled.jar \\\n    com.google.fhir.analytics.FhirEtl \\\n    --fhirServerUrl=http://localhost:8091/fhir \\\n    --resourceList=Patient,Encounter,Observation \\\n    --fhirDatabaseConfigPath=./utils/hapi-postgres-config.json \\\n    --jdbcModeEnabled=true --jdbcModeHapi=true \\\n    --jdbcMaxPoolSize=50 --jdbcFetchSize=1000 \\\n    --jdbcDriverClass=org.postgresql.Driver \\\n    --fhirSinkPath=http://localhost:8099/fhir \\\n    --sinkUserName=hapi --sinkPassword=hapi\n</code></pre>"},{"location":"installation_pipeline/#managing-and-scheduling-the-pipeline","title":"Managing and scheduling the Pipeline","text":"<p>The ETL Pipeline is designed as a stand-alone Apache Beam service. The Pipeline Controller module provides capabilities to help manage the Pipeline including: scheduling full versus incremental runs. It also provides some monitoring capabilities.</p>"},{"location":"installation_pipeline/#how-to-query-the-data-warehouse","title":"How to query the data warehouse","text":"<p>To query Parquet files, load them into a compatible data engine such as Apache Spark or use python in a jupyter notebook.</p> <p>The single machine Docker Compose configuration runs the pipeline and loads data into an Apache Spark Thrift server for you.</p>"},{"location":"release_process/","title":"Semantic versioning","text":"<p>Versioning across all Open Health Stack components is based on the major.minor.patch scheme and respects Semantic Versioning.</p> <p>Respecting Semantic Versioning is important for multiple reasons:</p> <ul> <li>It guarantees simple minor version upgrades, as long as you only use the   public APIs</li> <li>A new major version is an opportunity to thoroughly document breaking changes</li> <li>A new major/minor version is an opportunity to communicate new features   through a blog post</li> </ul>"},{"location":"release_process/#major-versions","title":"Major versions","text":"<p>The major version number is incremented on every breaking change.</p> <p>Whenever a new major version is released, we publish:</p> <ul> <li>a blog post with feature highlights, major bug fixes, breaking changes, and   upgrade instructions.</li> <li>an exhaustive changelog entry via the release notes</li> </ul>"},{"location":"release_process/#minor-versions","title":"Minor versions","text":"<p>The minor version number is incremented on every significant retro-compatible change.</p> <p>Whenever a new minor version is released, we publish:</p> <ul> <li>an exhaustive changelog entry via the release notes</li> </ul>"},{"location":"release_process/#patch-versions","title":"Patch versions","text":"<p>The patch version number is incremented on bugfixes releases.</p> <p>Whenever a new patch version is released, we publish:</p> <ul> <li>an exhaustive changelog entry</li> </ul>"},{"location":"support/","title":"Support","text":"<p>On this page we've listed some ways you can get technical support and Open Health Stack communities and forums that you can be a part of.</p> <p>Before participating please read our code of conduct that we expect all community members to adhere too.</p>"},{"location":"support/#developer-calls","title":"Developer calls","text":"<p>There are weekly Open Health Stack developer calls that you are welcome to join.</p> <ul> <li>Calls are on Thursdays and Alternate between Android FHIR SDK and OHS server   side (FHIR Data Pipes and Info Gateway)</li> <li>See the schedule below for more details</li> <li>To be added to the calls, please email: hello-ohs[at]google.com</li> </ul> <p>Developer call schedule</p> OHS Developers Call GMT East Africa India Android FHIR SDK 10:00 UK 12:00 Nairobi 14:30 Delhi Analytics and Info Gateway 13:00 UK 15:00 Nairobi 17:30 Delhi"},{"location":"support/#discussion-forums","title":"Discussion forums","text":"<p>We are in the process of setting up a dedicated discussion forum for Open Health Stack. In the meantime, you can reach out to hello-ohs[at]google.com</p>"},{"location":"support/#stack-overflow","title":"Stack Overflow","text":"<p>Stack Overflow is a popular forum to ask code-level questions or if you're stuck with a specific error. Read through the existing questions tagged with open-health-stack or fhir-data-pipes or ask your own!</p>"},{"location":"support/#bugs-or-feature-reqeusts","title":"Bugs or Feature reqeusts","text":"<p>Before submitting a bug or filing a feature reqeust, please review the open issues on our GitHub repository.</p> <p>If your issue is there, please add a comment. Otherwise, create a new issue to file a bug or submit a new feature request.</p> <p>Please review the contributing section.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>In this section you can access tutorials to learn about different aspects of working with FHIR Data Pipes.</p> <ul> <li>Single Machine Deployment: Great for getting started quickly and evaluating FHIR Data Pipes</li> <li>Visualize Parquet DWH with Apache Superset: With the Pipelines set-up, learn how to visualize the data using Apache Superset</li> <li>PostgreSQL with custom schema as DWH: See how to use the FHIR ViewDefinitions within the Pipelines to materialize views and load them into a PostgreSQL database</li> <li>Set up local test servers: Set up local HAPI FHIR server and load it with synthetic data </li> </ul>"},{"location":"tutorials/add_dashboard/","title":"Visualize Parquet DWH with Apache Superset","text":"<p>Generating dashboards is a common task for many digital health projects. </p> <p>This tutorial shows you how to use the popular open source Apache Superset package to build a dashboard on top of the \"lossless\" parquet DWH created via the FHIR Data Pipes Single Machine Deployment Tutorial.</p> <p>The principles here will apply to any visualization or BI tool that has a hive connector.</p>"},{"location":"tutorials/add_dashboard/#prerequisites","title":"Prerequisites","text":"<ul> <li>A FHIR Data Pipes single machine deployment using [local test     servers] as the data source</li> <li>Run the full pipeline at least once before following this guide.</li> </ul>"},{"location":"tutorials/add_dashboard/#install-apache-superset","title":"Install Apache Superset","text":"<p>There are several ways to install Superset. A simple option is to install Superset locally using docker-compose. Once it is set-up, log in by visiting http://localhost:8088/ and using the default credentials username: <code>admin</code> password: <code>admin</code>.</p>"},{"location":"tutorials/add_dashboard/#connect-to-the-apache-spark-sql-data-source","title":"Connect to the Apache Spark SQL data source","text":"<ol> <li>In the upper-right corner, select Settings &gt; Database Connections.</li> <li>In the upper-right corner, select + Database.</li> </ol>"},{"location":"tutorials/add_dashboard/#create-the-connection-for-spark-sql","title":"Create the connection for Spark SQL","text":"<ol> <li>Under Supported Databases, select Apache Spark SQL.</li> <li>Under Display Name, enter \"Single Machine Test Data\" or another name     your choice.</li> <li> <p>Under SQLAlchemy URI, enter the following:</p> <pre><code>hive://hive@&lt;IP_address_of_docker_network_gateway&gt;:10001\n</code></pre> <p>To find the IP address, run:</p> <pre><code>docker network inspect bridge --format='{{json .IPAM.Config}}'\n</code></pre> </li> <li> <p>Select Test Connection. You should see a pop-up in the bottom-right     corner that says \"Connection looks good!\"</p> <ul> <li>If you get an error, double-check the IP address is correct. You may     also need to install the Apache Spark SQL database drivers.</li> </ul> </li> <li>Select Connect. Although you see an error: \"An error occurred while     creating databases: Fatal error\" the connection is still created.</li> <li>Close the Connect a database window by clicking outside of it and     refresh the Databases page. The database you just created should appear.</li> </ol>"},{"location":"tutorials/add_dashboard/#create-a-dashboard","title":"Create a dashboard","text":"<p>Create an empty dashboard so you can add charts to it as they are created.</p> <ol> <li>At the top of the Superset page select the Dashboard tab, then click +     Dashboard.</li> <li>Select the title area which reads [ untitled dashboard ] and give the     dashboard a name. The examples below use \"Sample Dashboard\".</li> <li>In the upper-right corner, select Save.</li> </ol>"},{"location":"tutorials/add_dashboard/#query-the-database-and-generate-charts","title":"Query the database and generate Charts","text":"<ol> <li>From the top tabs, select SQL &gt; SQL Lab.</li> <li>If this is your first time using SQL Lab, you should be in a blank, empty     tab. If not, click New tab or press <code>Control+T</code> to create one.</li> <li>On the left side under Database, select the database you created     earlier, for example Single Machine Test Data.</li> </ol>"},{"location":"tutorials/add_dashboard/#add-a-big-number","title":"Add a Big Number","text":"<p>This first chart shows the total number of patients as a \"Big Number\".</p> <ol> <li> <p>Replace the placeholder query <code>SELECT ...</code> with the following:</p> <pre><code>SELECT COUNT(*) FROM Patient;\n</code></pre> </li> <li> <p>Click Run. After a moment, the results of the query should appear.</p> </li> <li>In the Results section, click Create Chart. This brings you to a new     chart page.</li> <li>Under the Chart Source pane, click Create a dataset and name it     \"Patient Count Dataset\".</li> <li>Under the Data tab, switch to the 4k (Big Number) chart type.</li> <li>In the Query section, click the Metric section and set the following     values:<ol> <li>Column: <code>count(1)</code></li> <li>Aggregate: <code>MAX</code></li> </ol> </li> <li>Click Update Chart at the bottom of the pane.</li> </ol> <p>The chart updates to show a single large number of the count of patients.</p> <ol> <li>Click Save to bring up the Save chart dialog. Name the chart     \"Patient Count\" and under Add to Dashboard select the dashboard you     created previously.</li> </ol>"},{"location":"tutorials/add_dashboard/#add-a-pie-chart","title":"Add a pie chart","text":"<p>The next chart shows the split of patients by gender as a pie chart.</p> <ol> <li>Navigate back to SQL Lab. You should see the previous patient count query.<ul> <li>If you'd like to keep the patient count query, make a new tab and set     the database to \"Single Machine Test Data\" or the name you chose.</li> </ul> </li> <li> <p>Enter the following as the query:</p> <pre><code>SELECT `gender` AS `gender`,\n       count(`gender`) AS `COUNT(gender)`\nFROM Patient AS P\nGROUP BY `gender`\nORDER BY `COUNT(gender)` DESC;\n</code></pre> </li> <li> <p>Click Run. After a moment, the results of the query should appear.</p> </li> <li>In the Results section, click Create Chart. This brings you to a new     chart page.</li> <li>Under the Chart Source pane, click Create a dataset and name it     \"Gender Split Dataset\".</li> <li>Under the Data tab, switch to the Pie Chart chart type.</li> <li>In the Query section, set the following values:<ul> <li>Dimensions:<ol> <li>Column: <code>gender</code></li> </ol> </li> <li>Metric:<ol> <li>Column: <code>COUNT(gender)</code></li> <li>Aggregate: <code>MAX</code></li> </ol> </li> </ul> </li> <li>Click Update Chart at the bottom of the pane. The chart updates to show     a pie chart of patients by gender.</li> <li>Click Save to bring up the Save chart dialog. Name the chart \"Gender     Split\" and under Add to Dashboard select the dashboard you created     previously.</li> </ol>"},{"location":"tutorials/add_dashboard/#add-a-complex-bar-chart","title":"Add a complex bar chart","text":"<p>The next chart shows number of patients on HIV treatment by year and display that as a bar chart over time. The query is provided below.</p> <ul> <li>Navigate back to SQL Lab. </li> <li>You should see the previous patient count query. </li> <li>If you'd like to keep the patient count query, make a new tab and set the database to \"Single Machine Test Data\" or the name you chose.</li> </ul> <p>Enter the following as the query as either a raw SQL-on-FHIR query or using the pre-defined observation_flat views:</p> Using SQL-on-FHIR QueryWith pre-defined flat views <pre><code>SELECT COUNT(*), YEAR(O.effective.dateTime)\nFROM Observation AS O LATERAL VIEW EXPLODE(code.coding) AS OCC LATERAL VIEW EXPLODE(O.value.codeableConcept.coding) AS OVCC\nWHERE OCC.code LIKE '1255%'\nAND OVCC.code LIKE \"1256%\"\nAND YEAR(O.effective.dateTime) &lt; 2023\nGROUP BY YEAR(O.effective.dateTime)\nORDER BY YEAR(O.effective.dateTime) ASC\n</code></pre> <pre><code>SELECT COUNT(*), YEAR(o.obs_date)\nFROM observation_flat as o\nWHERE o.code LIKE '1255%'\n    AND o.val_code LIKE \"1256%\"\n    AND YEAR(o.obs_date) &lt; 2023\nGROUP BY YEAR(o.obs_date)\nORDER BY YEAR(o.obs_date) ASC \n</code></pre> <p><code>The codes 1255% and 1266% refer to HIV\\_Tx codes in the dataset</code></p> <ul> <li>Click Run. After a moment, the results of the query should appear.</li> <li>In the Results section, click Create Chart. This brings you to a new     chart page.</li> <li>Under the Chart Source pane, click Create a dataset and name it \"HIV     Treatment Dataset\".</li> <li>Under the Data tab, switch to the Bar Chart chart type.</li> <li> <p>In the Query section, set the following values:</p> <p>X-Axis: Custom SQL: <code>YEAR(date\\_time)</code></p> <p>Metric: Column: <code>patient\\_id</code>, Aggregate: <code>COUNT</code></p> </li> <li> <p>Click Update Chart at the bottom of the pane. The chart updates to show     a bar chart of patients undergoing HIV treatment per year.</p> </li> <li>Click Save to bring up the Save chart dialog. Name the chart \"HIV     Treatment\" and under Add to Dashboard select the dashboard you created     previously.</li> </ul>"},{"location":"tutorials/add_dashboard/#edit-the-dashboard","title":"Edit the Dashboard","text":"<p>Navigate to the Dashboards section and select the Sample Dashboard you created. You should see something like this with the charts you created.</p> <p></p> <p>Create a dashboard with 2 rows and a header. First click Edit Dashboard then:</p> <ol> <li>In the right pane select the Layout Elements tab.</li> <li>Add a second row by dragging a Row element onto the canvas below the     existing row.</li> <li>Arrange the charts to have the Patient Count and Gender Split in the top     row, and HIV Treatment in the bottom row.</li> <li>Resize the top charts to use half of the width by dragging the edge of the     chart.</li> <li>Resize the HIV Treatment chart to take up the whole width of the row.</li> <li>Add a header element at the top and enter \"Key Program Metrics\".</li> </ol> <p>Your dashboard now looks something like this:</p> <p></p>"},{"location":"tutorials/add_dashboard/#update-the-dashboard","title":"Update the dashboard","text":"<p>To check that the dashboard is working properly, add a new patient to the FHIR Store and run the incremental pipeline.</p> <ol> <li> <p>Add a new patient to the server:</p> <pre><code>curl -X POST -H \"Content-Type: application/fhir+json; charset=utf-8\" \\\n     'http://localhost:8091/fhir/Patient/' -d '{\"resourceType\": \"Patient\"}'\n</code></pre> </li> <li> <p>Alternatively, use the HAPI FHIR server tester; go to the Patient resource     tester, click the CRUD Operations tab, paste the following resource     into the Contents field of the Create section, then click     Create.</p> <pre><code>{\"resourceType\": \"Patient\"}\n</code></pre> </li> <li> <p>Open the Pipeline Controller at http://localhost:8090 and run the     incremental pipeline.</p> </li> <li>Once the pipeline has finished, go to Superset and open your dashboard.     Click the ... button next to Edit Dashboard and then click Refresh     dashboard. You should see the patient count increase by 1.</li> </ol>"},{"location":"tutorials/add_dashboard/#learn-more","title":"Learn more","text":"<p>For more information about Superset, go to the Superset documentation.</p>"},{"location":"tutorials/gcp_fhirstore/","title":"Create a Google Cloud FHIR Store and BigQuery Dataset","text":"<p>One of the supported FHIR sinks is Google Cloud FHIR store and BigQuery. These instructions will guide you in creating a FHIR store and BigQuery dataset.</p> <p>To set up GCP project that you can use as a sink FHIR store:</p> <ul> <li>Create a new project in GCP. For an     overview of projects, datasets and data stores check     this document.</li> <li>Enable Google Cloud Healthcare API in the project and create a Google Cloud     Healthcare dataset</li> <li>Create a FHIR data store in the dataset with the R4 FHIR version</li> <li>Enable the BigQuery API in the project and dataset with the same name in the     project</li> <li>Download, install, and initialize the <code>gcloud</code> cli:     https://cloud.google.com/sdk/docs/quickstart</li> <li>Make sure you can authenticate with the project using the CLI:     https://developers.google.com/identity/sign-in/web/sign-in<ul> <li><code>$ gcloud init</code></li> <li><code>$ gcloud auth application-default login</code></li> <li>Create a service account for the project, generate a key, and save it     securely locally</li> <li>Add the <code>bigquery.dataEditor</code> and <code>bigquery.jobUser</code> roles to the     project in the <code>IAM &amp; Admin</code>/<code>Roles</code> settings or using the cli:<ul> <li><code>$ gcloud projects add-iam-policy-binding openmrs-260803 --role     roles/bigquery.admin --member     serviceAccount:openmrs-fhir-analytics@openmrs-260803.iam.gserviceaccount.com</code></li> <li><code>$ gcloud projects add-iam-policy-binding openmrs-260803 --role     roles/healthcare.datasetAdmin --member     serviceAccount:openmrs-fhir-analytics@openmrs-260803.iam.gserviceaccount.com</code></li> </ul> </li> <li>Activate the service account for your project using <code>gcloud auth     activate-service-account &lt;your-service-account&gt;     --key-file=&lt;your-key-file&gt; --project=&lt;your project&gt;</code></li> <li>Set the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable:     https://cloud.google.com/docs/authentication/getting-started#setting_the_environment_variable</li> </ul> </li> <li> <p>Use the script <code>utils/create_fhir_store.sh</code> to     create a FHIR store in this dataset which stream the changes to the BigQuery     dataset as well: <code>./utils/create_fhir_store.sh PROJECT LOCATION DATASET     FHIR-STORE-NAME</code></p> <ul> <li><code>PROJECT</code> is your GCP project.</li> <li><code>LOCATION</code> is GCP location where your dataset resides, e.g.,     <code>us-central1</code>.</li> <li><code>DATASET</code> is the name of the dataset you created.</li> <li><code>FHIR-STORE-NAME</code> is what it says.</li> </ul> <p>Note: If you get <code>PERMISSION_DENIED</code> errors, make sure to <code>IAM &amp; ADMIN</code>/<code>IAM</code>/<code>Members</code> and add the <code>bigquery.dataEditor</code> and <code>bigquery.jobUser</code> roles to the <code>Cloud Healthcare Service Agent</code> service account that shows up.</p> </li> </ul> <p>You can run the script with no arguments to see a sample usage. After you create the FHIR store, its full URL would be:</p> <p><code>https://healthcare.googleapis.com/v1/projects/PROJECT/locations/LOCATION/datasets/DATASET/fhirStores/FHIR-STORE-NAME</code></p>"},{"location":"tutorials/postgres_example/","title":"PostgreSQL DWH using custom schema","text":""},{"location":"tutorials/postgres_example/#overview","title":"Overview","text":"<p>In this tutorial you will learn how to configure and deploy FHIR Data Pipes to transform FHIR data into a PostgreSQL data-warehouse using FHIR ViewDefinition resources to define the custom schema.</p> <p>Requirements</p> <ul> <li>A source HAPI FHIR server configured to use Postgres as its database.</li> <li>If you don't have a server, use a local test server by following the instructions to bring up a source HAPI FHIR server with Postgres</li> <li>Docker: If you are using Linux, Docker must be in sudoless mode</li> <li>Docker Compose. This guide assumes you are using the latest version </li> <li>The FHIR Data Pipes repository, cloned onto the host machine</li> </ul>"},{"location":"tutorials/postgres_example/#configure-the-fhir-pipelines-controller","title":"Configure the FHIR Pipelines Controller","text":"<p>Note: All file paths are relative to the root of the FHIR Data Pipes repository.</p> <p>NOTE: You need to configure only one of the following options:</p> <ol> <li>For FHIR Search API (works for any FHIR server): </li> <li>Open <code>docker/config/application.yaml</code> and edit the value of <code>fhirServerUrl</code> to match the FHIR server you are connecting to. </li> <li> <p>Comment out the <code>dbConfig</code> in this case.</p> </li> <li> <p>For direct DB access (specific to HAPI FHIR servers):</p> </li> <li>Comment out <code>fhirServerUrl</code></li> <li>Set <code>dbConfig</code> to the DB connection config file, e.g., <code>docker/config/hapi-postgres-config_local.json</code>; </li> <li>Edit the values in this file to match the database for the FHIR server you are connecting to.</li> </ol>"},{"location":"tutorials/postgres_example/#set-the-sinkdbconfigpath","title":"Set the sinkDbConfigPath","text":"<p>The sinkDb refers to the target database that will become the data warehouse.</p> <p>With the default config, you will create both Parquet files (under <code>dwhRootPrefix</code>) and flattened views in the database configured by <code>sinkDbConfigPath</code> here. </p> <p>Make sure to create the database referenced in the connection config file (default is a postgreSQL db named 'views'). You can do this with the following SQL query:</p> <p><pre><code>CREATE DATABASE views;\n</code></pre> which you can run in Postgres like this: <pre><code>PGPASSWORD=admin psql -h 127.0.0.1 -p 5432 -U admin postgres -c \"CREATE DATABASE views\"\n</code></pre></p> <p>For documentation of all config parameters, see here.</p> <p>If you are using the local test servers, things should work with the default values. If not, use the IP address of the Docker default bridge network. To find it, run the following command and use the \"Gateway\" value:</p> <pre><code>docker network inspect bridge | grep Gateway\n</code></pre> <p>The Single Machine docker configuration uses two environment variables, <code>DWH_ROOT</code> and <code>PIPELINE_CONFIG</code>, whose default values are defined in the .env file. To override them, set the variable before running the <code>docker-compose</code> command. For example, to override the <code>DWH_ROOT</code> environment variable, run the following:</p> <pre><code>DWH_ROOT=\"$(pwd)/&lt;path_to_dwh_directory&gt;\" docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate \n</code></pre>"},{"location":"tutorials/postgres_example/#run-the-single-machine-configuration","title":"Run the Single Machine configuration","text":"<p>To bring up the <code>docker/compose-controller-spark-sql-single.yaml</code> configuration for the first time or if you have run this container in the past and want to include new changes pulled into the repo, run:</p> <pre><code>docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate --build\n</code></pre> <p>Alternatively, to run without rebuilding use:</p> <pre><code>docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate\n</code></pre> <p>Alternatively, <code>docker/compose-controller-spark-sql.yaml</code> serves as a very simple example on how to integrate the Parquet output of Pipelines in a Spark cluster environment.</p> <p>Once started, the Pipelines Controller is available at <code>http://localhost:8090</code> and the Spark Thrift server is at <code>http://localhost:10001</code>.</p> <p>The first time you run the Pipelines Controller, you must manually start a Full Pipeline run. In a browser go to <code>http://localhost:8090</code> and click the Run Full button. </p> <p>After running the Full Pipeline, use the Incremental Pipeline to update the Parquet files and tables. By default it is scheduled to run every hour, or you can manually trigger it.</p> <p>If the Incremental Pipeline does not work, or you see errors like:</p> <pre><code>ERROR o.openmrs.analytics.PipelineManager o.openmrs.analytics.PipelineManager$PipelineThread.run:343 - exception while running pipeline: \npipeline-controller    | java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n</code></pre> <p>try running <code>sudo chmod -R 755</code> on the Parquet file directory, by default located at <code>docker/dwh</code>.</p>"},{"location":"tutorials/postgres_example/#explore-the-resulting-schema-in-postgresql","title":"Explore the resulting schema in PostgreSQL","text":"<p>Connect to the PostgreSQL RDBMS via docker using the cmd: <code>docker exec -it &lt;container_name_or_id&gt; bash</code></p> <p>If using the default container (hapi-fhir-db) run: <code>docker exec -it hapi_fhir_db bash</code></p> <p>Using psql connect to the 'views'  database: <code>psql -U admin -d views</code></p> <p>To list the tables: <code>\\d</code>. It should look something like this:</p> Schema Name Type Owner public condition_flat table admin public diagnostic_report_flat table admin public immunization_flat table admin public location_flat table admin public medication_request_flat table admin public observation_flat table admin public organization_flat table admin public practitioner_flat table admin public practitioner_role_flat table admin public procedure_flat table admin"},{"location":"tutorials/postgres_example/#querying-the-database","title":"Querying the database","text":"<p>Let's do some basic quality checks to make sure the data is uploaded properly (note table names are case insensitive).</p> <p>Note: You will see that the number of patients and observations is higher than the count in the FHIR Server. This is due to the flattening</p> <p><pre><code>SELECT COUNT(0) FROM patient_flat;\n</code></pre> We should have exactly 114 patients: <pre><code>+-----------+\n| count     |\n+-----------+\n| 114       |\n+-----------+\n</code></pre></p> <p>Doing the same for observations: <pre><code>SELECT COUNT(0) FROM observation_flat;\n</code></pre> <pre><code>+-----------+\n| count  |\n+-----------+\n| 18343     |\n+-----------+\n</code></pre></p>"},{"location":"tutorials/single_machine/","title":"Single machine deployment tutorial","text":"<p>The repository includes a \"Single Machine\" Docker Compose configuration which brings up the FHIR Pipelines Controller plus a Spark Thrift server, letting you more easily run Spark SQL queries on the Parquet files output by the Pipelines Controller.</p> <p>To learn how the Pipelines Controller works on its own, Try out the FHIR Pipelines Controller.</p>"},{"location":"tutorials/single_machine/#requirements","title":"Requirements","text":"<ul> <li>A source HAPI FHIR server configured to use Postgres as its database<ul> <li>If you don't have a server, use a local test server by following the instructions to bring up a source HAPI FHIR server with Postgres</li> </ul> </li> <li>Docker<ul> <li>If you are using Linux, Docker must be in sudoless mode</li> </ul> </li> <li>Docker Compose - this guide assumes you are using the latest version </li> <li>The FHIR Data Pipes repository, cloned onto the host machine</li> </ul>"},{"location":"tutorials/single_machine/#configure-the-fhir-pipelines-controller","title":"Configure the FHIR Pipelines Controller","text":"<p>Note: All file paths are relative to the root of the FHIR Data Pipes repository.</p> <p>NOTE: You need to configure only one of the following options:</p> <ol> <li>For FHIR Search API (works for any FHIR server): </li> <li>Open <code>docker/config/application.yaml</code> and edit the value of <code>fhirServerUrl</code> to match the FHIR server you are connecting to. </li> <li> <p>Comment out the <code>dbConfig</code> in this case.</p> </li> <li> <p>For direct DB access (specific to HAPI FHIR servers):</p> </li> <li>Comment out <code>fhirServerUrl</code></li> <li>Set <code>dbConfig</code> to the DB connection config file, e.g., <code>docker/config/hapi-postgres-config_local.json</code>; </li> <li>Edit the values in this file to match the database for the FHIR server you are connecting to.</li> </ol>"},{"location":"tutorials/single_machine/#flattened-views","title":"Flattened views","text":"<p>With the default config, you will create both Parquet files (under <code>dwhRootPrefix</code>) and flattened views in the database configured by <code>sinkDbConfigPath</code> here.  * If you don't need flattened views you can comment out that setting.  * If you do need them, make sure you create the DB referenced in the connection config file, e.g., with the following SQL query:</p> <p><pre><code>CREATE DATABASE views;\n</code></pre> which you can run in Postgres like this: <pre><code>PGPASSWORD=admin psql -h 127.0.0.1 -p 5432 -U admin postgres -c \"CREATE DATABASE views\"\n</code></pre></p> <p>For documentation of all config parameters, see here.</p> <p>If you are using the local test servers, things should work with the default values. If not, use the IP address of the Docker default bridge network. To find it, run the following command and use the \"Gateway\" value:</p> <pre><code>docker network inspect bridge | grep Gateway\n</code></pre> <p>The Single Machine docker configuration uses two environment variables, <code>DWH_ROOT</code> and <code>PIPELINE_CONFIG</code>, whose default values are defined in the .env file. To override them, set the variable before running the <code>docker-compose</code> command. For example, to override the <code>DWH_ROOT</code> environment variable, run the following:</p> <pre><code>DWH_ROOT=\"$(pwd)/&lt;path_to_dwh_directory&gt;\" docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate \n</code></pre>"},{"location":"tutorials/single_machine/#run-the-single-machine-configuration","title":"Run the Single Machine configuration","text":"<p>To bring up the <code>docker/compose-controller-spark-sql-single.yaml</code> configuration for the first time or if you have run this container in the past and want to include new changes pulled into the repo, run:</p> <pre><code>docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate --build\n</code></pre> <p>Alternatively, to run without rebuilding use:</p> <pre><code>docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate\n</code></pre> <p>Alternatively, <code>docker/compose-controller-spark-sql.yaml</code> serves as a very simple example on how to integrate the Parquet output of Pipelines in a Spark cluster environment.</p> <p>Once started, the Pipelines Controller is available at <code>http://localhost:8090</code> and the Spark Thrift server is at <code>http://localhost:10001</code>.</p> <p>The first time you run the Pipelines Controller, you must manually start a Full Pipeline run. In a browser go to <code>http://localhost:8090</code> and click the Run Full button. </p> <p>After running the Full Pipeline, use the Incremental Pipeline to update the Parquet files and tables. By default it is scheduled to run every hour, or you can manually trigger it.</p> <p>If the Incremental Pipeline does not work, or you see errors like:</p> <pre><code>ERROR o.openmrs.analytics.PipelineManager o.openmrs.analytics.PipelineManager$PipelineThread.run:343 - exception while running pipeline: \npipeline-controller    | java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n</code></pre> <p>try running <code>sudo chmod -R 755</code> on the Parquet file directory, by default located at <code>docker/dwh</code>.</p>"},{"location":"tutorials/single_machine/#view-and-analyze-the-data-using-spark-thrift-server","title":"View and analyze the data using Spark Thrift server","text":"<p>Connect to the Spark Thrift server using a client that supports Apache Hive. For example, if using the JDBC driver, the URL should be <code>jdbc:hive2://localhost:10001</code>. The pipeline will automatically create <code>Patient</code>, <code>Encounter</code>, and <code>Observation</code> tables when run.</p> <p>Let's do some basic quality checks to make sure the data is uploaded properly (note table names are case insensitive):</p> <p><pre><code>SELECT COUNT(0) FROM Patient;\n</code></pre> We should have exactly 79 patients: <pre><code>+-----------+\n| count(0)  |\n+-----------+\n| 79        |\n+-----------+\n</code></pre></p> <p>Doing the same for observations: <pre><code>SELECT COUNT(0) FROM Observation;\n</code></pre> <pre><code>+-----------+\n| count(0)  |\n+-----------+\n| 17279     |\n+-----------+\n</code></pre></p>"},{"location":"tutorials/single_machine/#whats-next","title":"What's next","text":"<p>Now that the data is available in an SQL queryable format, you can start to explore it using SQL or jupyter notebooks or build dashboards using common open source tools like Apache SuperSet.</p>"},{"location":"tutorials/test_servers/","title":"Set-up Local Test Servers","text":"<p>This guide shows how to use provided Docker images to bring up test servers (and optionally load with synthetic data) to easily get started the FHIR Data Pipes pipelines. </p> <p>There are 3 Docker server configurations you can use for testing:</p> <ul> <li>HAPI FHIR server with Postgres (source)</li> <li>OpenMRS Reference Application with MySQL (source)</li> <li>HAPI FHIR server (destination)</li> </ul>"},{"location":"tutorials/test_servers/#instructions","title":"Instructions","text":"<p>Note: All commands are run from the root directory of the repository.</p> <ol> <li> <p>Create an external Docker network named <code>cloudbuild</code>:</p> <pre><code>docker network create cloudbuild\n</code></pre> </li> <li> <p>Bring up a FHIR source server for the pipeline. This can be either:</p> <ul> <li> <p>HAPI FHIR server with     Postgres:</p> <pre><code>docker-compose  -f ./docker/hapi-compose.yml up  --force-recreate -d\n</code></pre> <p>The base FHIR URL for this server is <code>http://localhost:8091/fhir</code>. If you get a CORS error when accessing the URL, try manually refreshing (e.g. ctrl-shift-r).</p> </li> <li> <p>OpenMRS Reference Application with     MySQL:</p> <pre><code>docker-compose -f ./docker/openmrs-compose.yml up --force-recreate -d\n</code></pre> <p>The base FHIR URL for this server is <code>http://localhost:8099/openmrs/ws/fhir2/R4</code></p> </li> </ul> </li> <li> <p>Upload the synthetic data stored in     sample_data     to the FHIR server that you brought up using the Synthea data     uploader.</p> <p>The uploader requires the <code>google-auth</code> Python library, which you can install using:</p> <pre><code>pip3 install --upgrade google-auth\n</code></pre> <p>For example, to upload to the HAPI FHIR server brought up in the previous step, run:</p> <pre><code>python3 ./synthea-hiv/uploader/main.py HAPI http://localhost:8091/fhir \\\n--input_dir ./synthea-hiv/sample_data --cores 8\n</code></pre> <p>Depending on your machine, using too many cores may slow down your machine or cause JDBC connection pool errors with the HAPI FHIR server. Reducing the number of cores using the <code>--cores</code> flag should help at the cost of increasing the time to upload the data.</p> </li> <li> <p>(optional) If you only want to output Apache Parquet files, there is no additional     setup. If you want to test outputting to another FHIR server, then bring up a     destination HAPI FHIR     server:</p> <pre><code>docker-compose  -f ./docker/sink-compose.yml up  --force-recreate -d\n</code></pre> <p>The base URL for this server is <code>http://localhost:8098/fhir</code>.</p> </li> </ol>"},{"location":"tutorials/test_servers/#additional-notes-for-openmrs","title":"Additional notes for OpenMRS","text":"<p>Once running you can access OpenMRS at http://localhost:8099/openmrs/ using username \"admin\" and password \"Admin123\". The Docker image includes the required FHIR2 module and demo data. Edit <code>docker/openmrs-compose.yaml</code> to change the default port.</p> <p>Note: If <code>docker-compose</code> fails, you may need to adjust file permissions. In particular if the permissions on <code>mysqld.cnf</code> is not right, the <code>datadir</code> set in this file will not be read by MySQL and it will cause OpenMRS to require its <code>initialsetup</code> (which is not needed since the MySQL image already has all the data and tables needed):</p> <pre><code>$ docker-compose -f docker/openmrs-compose.yaml down -v\n$ chmod a+r docker/mysql-build/mysqld.cnf\n$ chmod -R a+r ./utils\n$ docker-compose -f docker/openmrs-compose.yaml up\n</code></pre> <p>In order to see the demo data in OpenMRS you must rebuild the search index. In OpenMRS go to Home &gt; System Administration &gt; Advanced Administration. Under Maintenance go to Search Index then Rebuild Search Index.</p>"},{"location":"tutorials/try_controller/","title":"Try the Pipelines Controller with HAPI FHIR Store","text":"<p>The FHIR Pipelines Controller makes it easy to schedule and manage the transformation of data from a HAPI FHIR server to a collection of Apache Parquet files. It uses FHIR Data Pipes JDBC pipeline to run either full or incremental transformations to a Parquet data warehouse. </p> <p>The FHIR Pipelines Controller only works with HAPI FHIR servers using Postgres. You can see an example of configuring a HAPI FHIR server to use Postgres here. </p> <p>This guide will show you how to set up the FHIR Pipelines Controller with a test HAPI FHIR server. It assumes you are using Linux, but should work with other environments with minor adjustments.</p>"},{"location":"tutorials/try_controller/#clone-the-fhir-data-pipes-repository","title":"Clone the fhir-data-pipes repository","text":"<p>Clone the fhir-data-pipes GitHub repository using your preferred method. After cloned, open a terminal window and <code>cd</code> to the directory where you cloned it. Later terminal commands will assume your working directory is the repository root.</p>"},{"location":"tutorials/try_controller/#set-up-the-test-server","title":"Set up the test server","text":"<p>The repository includes a Docker Compose configuration to bring up a HAPI FHIR server configured to use Postgres.</p> <p>To set up the test server, follow these instructions. At step two, follow the instructions for \"HAPI source server with Postgres\".</p>"},{"location":"tutorials/try_controller/#configure-the-fhir-pipelines-controller","title":"Configure the FHIR Pipelines Controller","text":"<p>First, open <code>pipelines/controller/config/application.yml</code> in a text editor.</p> <p>Change fhirServerUrl to be:</p> <pre><code>fhirServerUrl: \"http://localhost:8091/fhir\"\n</code></pre> <p>Read through the rest of the file to see other settings. The other lines may remain the same. Note the value of <code>dwhRootPrefix</code>, as it will be where the Parquet files are written. You can also adjust this value if desired. Save and close the file.</p> <p>Next, open <code>pipelines/controller/config/hapi-postgres-config.json</code> in a text editor.</p> <p>Change <code>databaseHostName</code> to be:</p> <pre><code>\"databaseHostName\" : \"localhost\"\n</code></pre> <p>Save and close the file.</p>"},{"location":"tutorials/try_controller/#run-the-fhir-pipelines-controller","title":"Run the FHIR Pipelines Controller","text":"<p>From the terminal run:</p> <pre><code>cd pipelines/controller/\nmvn spring-boot:run\n</code></pre> <p>Open a web browser and visit http://localhost:8080. You should see the FHIR Pipelines Control Panel.</p> <p></p> <p>Before automatic incremental runs can occur, you must manually trigger a full run. Under the Run Full Pipeline section, click on Run Full. Wait for the run to complete. </p>"},{"location":"tutorials/try_controller/#explore-the-configuration-settings","title":"Explore the configuration settings","text":"<p>The Control Panel shows the options being used by the FHIR Pipelines Controller.</p>"},{"location":"tutorials/try_controller/#main-configuration-parameters","title":"Main configuration parameters","text":"<p>This section corresponds to the settings in the <code>application.yml</code> file.</p>"},{"location":"tutorials/try_controller/#batch-pipeline-non-default-configurations","title":"Batch pipeline non-default configurations","text":"<p>This section calls out FHIR Data Pipes batch pipeline settings that are different from their default values. These are also mostly derived from <code>application.yml</code>. Use these settings if you want to run the batch pipeline manually.</p>"},{"location":"tutorials/try_controller/#query-the-dwh","title":"Query the DWH","text":"<p>On your machine, look for the Parquet files created in the directory specified by <code>dwhRootPrefix</code> in the application.yml file. You can explore the data using a Jupyter notebook with pyspark and pandas libraries installed</p> <p>Alternatively you can load the Parquet into an SQL Query Engine like SparkSQL. See the tutorials section for more.</p>"}]}