# Resource performance monitoring and graphing module

This is a module for monitoring the resource usage of the batch pipeline with [HAPI FHIR](https://hapifhir.io/) 
as the source. The scripts in this module generate graphs and csv files of resource usage 
(CPU, memory and I/O) of the HAPI server, PostgreSQL database and pipeline over the duration of the 
batch job. The user is able to specify the number of processes/cores used in the pipeline to assess 
the batch pipeline's performance on the local machine.

## Prerequisites

-   Ensure you are able to run the `docker` command 
[as a non-root user](https://docs.docker.com/engine/install/linux-postinstall/). If not, run  
`sudo chmod 666 /var/run/docker.sock`
-   A [HAPI](https://hapifhir.io/) instance with a [PostgreSQL](https://www.postgresql.org/) instance 
as its database. For convenience, [`hapi-compose.yml`](/docker/hapi-compose.yml) file in the in the 
[`docker`](/docker) directory contains images of a HAPI server using postrges as its database. Start 
the docker container with  
`docker-compose -f hapi-compose.yml up`
-   To upload sample data in the HAPI sever, please refer to this [`document`](/synthea-hiv/README.md).

## Graphing module

The graphing module, [graph_pidstat.py](/utils/resource-monitor/graph_pidstat.py), can be used with 
the following parameters:  
`--numProc` - The number of processes/cores to use in the batch job.    
`--dataDescription` - A description of the data/job.  
`--outputParquetPath` - The output path for the parquet files generated by the batch job.  
`--outputResultsPath` - The output path for the graphs and csv files.  

Example usage:  
```shell
python3 graph_pidstat.py \
      --numProc 12  --dataDescription small_dataset \
      --outputParquetPath /tmp/hapi-test/  --outputResultsPath /tmp/hapi-performance/
```

## Ouput graphs

The output graphs can be found in the output results directory specified by the user; 
the default is `/tmp/hapi-performance/`. The graphs visualize the CPU, memory and I/O usage of the 
HAPI server, PostgreSQL database and pipeline over the duration of the batch job.

## Automating runs with different number of cores

The bash script, [auto.sh](/utils/resource-monitor/auto.sh), can be used to automate the generation 
of resource usage graphs and csv files with different number of cores used in the batch job.

Example usage:  
```shell
sh auto.sh [data description] [output parquet path] [output results path] [num cores increment] [lower core limit] [upper core limit]
